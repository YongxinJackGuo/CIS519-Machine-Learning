{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIO3UIZe6wsZ"
   },
   "source": [
    "# CIS 419/519 \n",
    "#**Homework 3 : Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gS022EH9_-p"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjPfIJ5G52It"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UodjntNc6Ex2"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, alpha = 0.01, regLambda=0.01, regNorm=2, epsilon=0.0001, maxNumIters = 10000, initTheta = None):\n",
    "        '''\n",
    "        Constructor\n",
    "        Arguments:\n",
    "        \talpha is the learning rate\n",
    "        \tregLambda is the regularization parameter\n",
    "        \tregNorm is the type of regularization (either L1 or L2, denoted by a 1 or a 2)\n",
    "        \tepsilon is the convergence parameter\n",
    "        \tmaxNumIters is the maximum number of iterations to run\n",
    "          initTheta is the initial theta value. This is an optional argument\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.regLambda = regLambda\n",
    "        self.regNorm = regNorm\n",
    "        self.epsilon = epsilon\n",
    "        self.maxNumIters = maxNumIters\n",
    "        self.theta = initTheta\n",
    "        self.costList = []\n",
    "    \n",
    "    def reset_values(self):\n",
    "        self.theta = None\n",
    "        self.costList = []\n",
    "        \n",
    "    def computeCost(self, theta, X, y, regLambda):\n",
    "        '''\n",
    "        Computes the objective function\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-by-1 numpy matrix\n",
    "            regLambda is the scalar regularization constant\n",
    "        Returns:\n",
    "            a scalar value of the cost  ** make certain you're not returning a 1 x 1 matrix! **\n",
    "        '''\n",
    "        n, d = X.shape\n",
    "        h_theta = self.sigmoid(np.dot(X, theta))\n",
    "        h_theta = np.array(h_theta)\n",
    "        cost = -(np.dot(y.T, np.log(h_theta)) + np.dot((1 - y).T, np.log(1 - h_theta)))\n",
    "        if self.regNorm is 1:\n",
    "            regCost = regLambda * sum(abs(theta))\n",
    "        if self.regNorm is 2:\n",
    "            regCost = regLambda * ((np.linalg.norm(theta)) ** 2)\n",
    "        return cost + regCost\n",
    "\n",
    "    \n",
    "    \n",
    "    def computeGradient(self, theta, X, y, regLambda):\n",
    "        '''\n",
    "        Computes the gradient of the objective function\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-by-1 numpy matrix\n",
    "            regLambda is the scalar regularization constant\n",
    "        Returns:\n",
    "            the gradient, an d-dimensional vector\n",
    "        '''\n",
    "        h_theta = self.sigmoid(np.dot(X, theta))\n",
    "        #print('The h_theta is: ', h_theta)\n",
    "        n, d = X.shape\n",
    "        gradient = np.zeros((d,1))\n",
    "        h_theta = np.array(h_theta)\n",
    "        gradient[0,0] = sum(h_theta - y) # no regularization for the x_i0\n",
    "        for j in range(d-1):\n",
    "            if self.regNorm is 1:\n",
    "                gradient[j+1,0] = np.dot(X[:,j+1].reshape((1,n)), (h_theta - y)) + regLambda * ( theta[j+1, 0] / abs(theta[j+1, 0]) )\n",
    "            if self.regNorm is 2:\n",
    "                gradient[j+1,0] = np.dot(X[:,j+1].reshape((1,n)), (h_theta - y)) + regLambda * theta[j+1, 0]\n",
    "        #print('gradient: ', gradient)\n",
    "        return gradient\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains the model\n",
    "        Arguments:\n",
    "            X is a n-by-d Pandas data frame\n",
    "            y is an n-by-1 Pandas data frame\n",
    "        Note:\n",
    "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
    "            Standardization should be optionally done before fit() is called.\n",
    "        '''\n",
    "        n = len(y)\n",
    "        X = X.to_numpy()\n",
    "        X = np.c_[np.ones((n, 1)), X]\n",
    "        \n",
    "        n, d = X.shape\n",
    "        y = y.to_numpy()\n",
    "        y = y.reshape(n, 1)\n",
    "        \n",
    "        # start doing gradient descent\n",
    "        if self.theta is None:\n",
    "            #self.theta = np.matrix(np.zeros((d,1))) #np.zeros(d)\n",
    "            self.theta = np.matrix(np.random.rand(d, 1) - 0.5)\n",
    "\n",
    "        \n",
    "        count = 0\n",
    "        self.costList.append((None, 100)) # assign a large dummy theta and assign None to cost as the starter \n",
    "        while not self.hasConverged(self.theta, self.costList[count][1]):\n",
    "            self.costList.append( (self.computeCost(self.theta, X, y, self.regLambda), self.theta) )\n",
    "            count = count + 1\n",
    "            #print(\"Iteration: \", count, \" Cost: \", self.costList[count][0], \" Theta.T: \", self.theta.T)\n",
    "            self.theta = self.theta - self.alpha * self.computeGradient(self.theta, X, y, self.regLambda)\n",
    "            #print('the maxNumInters is: ', self.maxNumIters, ' the count is: ', count)\n",
    "            if count > self.maxNumIters:\n",
    "                break\n",
    "            \n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Used the model to predict values for each instance in X\n",
    "        Arguments:\n",
    "            X is a n-by-d Pandas data frame\n",
    "        Returns:\n",
    "            an n-by-1 dimensional Pandas data frame of the predictions\n",
    "        Note:\n",
    "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
    "            Standardization should be optionally done before predict() is called.\n",
    "        '''\n",
    "        prob = self.predict_proba(X)\n",
    "        label = pd.DataFrame((prob >= 0.5))\n",
    "        return label\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Used the model to predict the class probability for each instance in X\n",
    "        Arguments:\n",
    "            X is a n-by-d Pandas data frame\n",
    "        Returns:\n",
    "            an n-by-1 Pandas data frame of the class probabilities\n",
    "        Note:\n",
    "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
    "            Standardization should be optionally done before predict_proba() is called.\n",
    "        '''\n",
    "        n, d = X.shape\n",
    "        X = X.to_numpy()\n",
    "        X = np.c_[np.ones((n, 1)), X]\n",
    "        prob = pd.DataFrame(self.sigmoid(np.dot(X, self.theta)))\n",
    "        return prob\n",
    "    \n",
    "    def hasConverged(self, theta, prevTheta):\n",
    "        running_epsilon = np.linalg.norm( theta - prevTheta)\n",
    "        #print('The convergence now is: ', running_epsilon)\n",
    "        return (running_epsilon < self.epsilon)\n",
    "\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        '''\n",
    "        Computes the sigmoid function 1/(1+exp(-z))\n",
    "        '''\n",
    "        threshold_upper = 30\n",
    "        threshold_lower = -30\n",
    "        Z[Z > threshold_upper] = threshold_upper\n",
    "        Z[Z < threshold_lower] = threshold_lower\n",
    "        #print('sigmoid value: ', 1/(1 + np.exp(-Z)))\n",
    "        return 1/(1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Y-_IFEK6g4Q"
   },
   "source": [
    "# Test Logistic Regression 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_7Un3fMJ6keB"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-1f25df9199a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtest_logreg1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-108-1f25df9199a7>\u001b[0m in \u001b[0;36mtest_logreg1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mregNorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mlogregModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregLambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregLambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregNorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregNorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mlogregModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXstandardized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Plot the decision boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-97e96a0c63c7>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcostList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# assign a large dummy theta and assign None to cost as the starter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasConverged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcostList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcostList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregLambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m#print(\"Iteration: \", count, \" Cost: \", self.costList[count][0], \" Theta.T: \", self.theta.T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-97e96a0c63c7>\u001b[0m in \u001b[0;36mcomputeCost\u001b[0;34m(self, theta, X, y, regLambda)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregNorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mregCost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregLambda\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mregCost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__array_finalize__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test script for training a logistic regressiom model\n",
    "#\n",
    "# This code should run successfully without changes if your implementation is correct\n",
    "#\n",
    "from numpy import loadtxt, ones, zeros, where\n",
    "import numpy as np\n",
    "from pylab import plot,legend,show,where,scatter,xlabel, ylabel,linspace,contour,title\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_logreg1():\n",
    "    # load the data\n",
    "    filepath = \"http://www.seas.upenn.edu/~cis519/spring2020/data/hw3-data1.csv\"\n",
    "    df = pd.read_csv(filepath, header=None)\n",
    "\n",
    "    X = df[df.columns[0:2]]\n",
    "    y = df[df.columns[2]]\n",
    "\n",
    "    n,d = X.shape\n",
    "    \n",
    "    # # Standardize features\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    standardizer = StandardScaler()\n",
    "    Xstandardized = pd.DataFrame(standardizer.fit_transform(X))  # compute mean and stdev on training set for standardization\n",
    "    \n",
    "    # train logistic regression\n",
    "    regLambda = 50#0.00000001\n",
    "    regNorm = 1\n",
    "    logregModel = LogisticRegression(regLambda = regLambda, regNorm = regNorm, epsilon = 0.001)\n",
    "    logregModel.fit(Xstandardized,y)\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min = X[X.columns[0]].min() - .5\n",
    "    x_max = X[X.columns[0]].max() + .5\n",
    "    y_min = X[X.columns[1]].min() - .5\n",
    "    y_max = X[X.columns[1]].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    allPoints = pd.DataFrame(np.c_[xx.ravel(), yy.ravel()])\n",
    "    allPoints = pd.DataFrame(standardizer.transform(allPoints))\n",
    "    Z = logregModel.predict(allPoints)\n",
    "    Z = np.asmatrix(Z.to_numpy())\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1, figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "    # Plot the training points\n",
    "    plt.scatter(X[X.columns[0]], X[X.columns[1]], c=y.ravel(), edgecolors='k', cmap=plt.cm.Paired)\n",
    "    \n",
    "    # Configure the plot display\n",
    "    plt.xlabel('Exam 1 Score')\n",
    "    plt.ylabel('Exam 2 Score')\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.savefig('Logreg Decision Boundary with lambda ' + str(regLambda) + ' under L' + str(regNorm))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "test_logreg1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_9HL_cUX6NYm"
   },
   "source": [
    "# Map Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z7uldP0A6Hcn"
   },
   "outputs": [],
   "source": [
    "def mapFeature(X, column1, column2, maxPower = 6):\n",
    "    '''\n",
    "    Maps the two specified input features to quadratic features. Does not standardize any features.\n",
    "        \n",
    "    Returns a new feature array with d features, comprising of\n",
    "        X1, X2, X1 ** 2, X2 ** 2, X1*X2, X1*X2 ** 2, ... up to the maxPower polynomial\n",
    "        \n",
    "    Arguments:\n",
    "        X is an n-by-d Pandas data frame, where d > 2\n",
    "        column1 is the string specifying the column name corresponding to feature X1\n",
    "        column2 is the string specifying the column name corresponding to feature X2\n",
    "    Returns:\n",
    "        an n-by-d2 Pandas data frame, where each row represents the original features augmented with the new features of the corresponding instance\n",
    "    '''\n",
    "    X_1_poly = pd.DataFrame(X.iloc[:, 0])\n",
    "    X_2_poly = pd.DataFrame(X.iloc[:, 1])\n",
    "    i, j = X.shape\n",
    "    for d in range(maxPower-1):\n",
    "            X_1_poly = pd.concat([X_1_poly, X.iloc[:, 0] * X_1_poly.iloc[:, d]], axis = 1)\n",
    "            X_2_poly = pd.concat([X_2_poly, X.iloc[:, 1] * X_2_poly.iloc[:, d]], axis = 1)\n",
    "    X_1_poly = pd.concat([pd.DataFrame(np.ones((i, 1))), X_1_poly], axis = 1)\n",
    "    \n",
    "    instance, degree_with_one = X_1_poly.shape\n",
    "    degree_without_one = degree_with_one - 1\n",
    "    \n",
    "    X_mapped = pd.DataFrame()\n",
    "    X_mapped = pd.concat([X_mapped, X_1_poly], axis = 1)\n",
    "    \n",
    "    for d in range(degree_with_one):\n",
    "        for complement in range(degree_without_one - d):\n",
    "            X_mapped = pd.concat([X_mapped, X_1_poly.iloc[:, d] * X_2_poly.iloc[:, complement]], axis = 1)\n",
    "    return X_mapped\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcBEI53O6lde"
   },
   "source": [
    "# Test Logistic Regression 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGqdTWMU6oDH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guoyongxin/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/ipykernel_launcher.py:68: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-b51b7f10d7a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mtest_logreg2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-b51b7f10d7a4>\u001b[0m in \u001b[0;36mtest_logreg2\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# train logistic regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mlogregModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionAdagrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregLambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregNorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxNumIters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mlogregModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXaug\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Plot the decision boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-2f3ec729cd62>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0malpha_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mprev_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;31m# store the previous theta values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha_t\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregLambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;31m# print('Iteration ', count, ' theta: ', self.theta.T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-2f3ec729cd62>\u001b[0m in \u001b[0;36mcomputeGradient\u001b[0;34m(self, theta, X, y, regLambda)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregNorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mgradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_theta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mregLambda\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregNorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mgradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_theta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mregLambda\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from numpy import loadtxt, ones, zeros, where\n",
    "import numpy as np\n",
    "from pylab import plot,legend,show,where,scatter,xlabel, ylabel,linspace,contour,title\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_logreg2():\n",
    "\n",
    "    polyPower = 6\n",
    "\n",
    "    # load the data\n",
    "    filepath = \"http://www.seas.upenn.edu/~cis519/spring2020/data/hw3-data2.csv\"\n",
    "    df = pd.read_csv(filepath, header=None)\n",
    "\n",
    "    X = df[df.columns[0:2]]\n",
    "    y = df[df.columns[2]]\n",
    "\n",
    "    n,d = X.shape\n",
    "\n",
    "    # map features into a higher dimensional feature space\n",
    "    Xaug = mapFeature(X.copy(), X.columns[0], X.columns[1], polyPower)\n",
    "\n",
    "    # # Standardize features\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    standardizer = StandardScaler()\n",
    "    Xaug = pd.DataFrame(standardizer.fit_transform(Xaug))  # compute mean and stdev on training set for standardization\n",
    "    \n",
    "    # train logistic regression\n",
    "    logregModel = LogisticRegressionAdagrad(regLambda = 0.0001, regNorm=1, epsilon=0.001, maxNumIters = 10000)\n",
    "    logregModel.fit(Xaug,y)\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min = X[X.columns[0]].min() - .5\n",
    "    x_max = X[X.columns[0]].max() + .5\n",
    "    y_min = X[X.columns[1]].min() - .5\n",
    "    y_max = X[X.columns[1]].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    allPoints = pd.DataFrame(np.c_[xx.ravel(), yy.ravel()])\n",
    "    allPoints = mapFeature(allPoints, allPoints.columns[0], allPoints.columns[1], polyPower)\n",
    "    allPoints = pd.DataFrame(standardizer.transform(allPoints))\n",
    "    Xaug = pd.DataFrame(standardizer.fit_transform(Xaug))  # standardize data\n",
    "    \n",
    "    Z = logregModel.predict(allPoints)\n",
    "    Z = np.asmatrix(Z.to_numpy())\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1, figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "    # Plot the training points\n",
    "    plt.scatter(X[X.columns[0]], X[X.columns[1]], c=y.ravel(), edgecolors='k', cmap=plt.cm.Paired)\n",
    "    \n",
    "    # Configure the plot display\n",
    "    plt.xlabel('Microchip Test 1')\n",
    "    plt.ylabel('Microchip Test 2')\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    \n",
    "    plt.savefig('Nonlinear_L1_1e_04')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(str(Z.min()) + \" \" + str(Z.max()))\n",
    "\n",
    "test_logreg2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z7ef6eUW7BNy"
   },
   "source": [
    "# Logistic Regression with Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5zcisRww7Y3X"
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionAdagrad:\n",
    "\n",
    "    def __init__(self, alpha = 0.1, regLambda=0.01, regNorm=2, epsilon=0.00001, maxNumIters = 10000, initTheta = None):\n",
    "        '''\n",
    "        Constructor\n",
    "        Arguments:\n",
    "        \talpha is the learning rate\n",
    "        \tregLambda is the regularization parameter\n",
    "        \tregNorm is the type of regularization (either L1 or L2, denoted by a 1 or a 2)\n",
    "        \tepsilon is the convergence parameter\n",
    "        \tmaxNumIters is the maximum number of iterations to run\n",
    "          initTheta is the initial theta value. This is an optional argument\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.regLambda = regLambda\n",
    "        self.regNorm = regNorm\n",
    "        self.epsilon = epsilon\n",
    "        self.maxNumIters = maxNumIters\n",
    "        self.theta = initTheta\n",
    "        self.costList = []\n",
    "        self.Xi = 1e-05\n",
    "    \n",
    "    def reset_values(self):\n",
    "        self.theta = None\n",
    "        self.costList = []\n",
    "    \n",
    "    def computeCost(self, theta, X, y, regLambda):\n",
    "        '''\n",
    "        Computes the objective function\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-by-1 numpy matrix\n",
    "            regLambda is the scalar regularization constant\n",
    "        Returns:\n",
    "            a scalar value of the cost  ** make certain you're not returning a 1 x 1 matrix! **\n",
    "        '''\n",
    "        d = len(X)\n",
    "        h_theta = self.sigmoid(np.dot(X.reshape((1, d)), theta))\n",
    "        #h_theta = np.array(h_theta)\n",
    "        cost = -(np.dot(y.T, np.log(h_theta)) + np.dot((1 - y).T, np.log(1 - h_theta)))\n",
    "        if self.regNorm is 1:\n",
    "            regCost = regLambda * sum(abs(theta))\n",
    "        if self.regNorm is 2:\n",
    "            regCost = regLambda * ((np.linalg.norm(theta)) ** 2)\n",
    "        return cost + regCost\n",
    "\n",
    "    \n",
    "    \n",
    "    def computeGradient(self, theta, X, y, regLambda):\n",
    "        '''\n",
    "        Computes the gradient of the objective function\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-by-1 numpy matrix\n",
    "            regLambda is the scalar regularization constant\n",
    "        Returns:\n",
    "            the gradient, an d-dimensional vector\n",
    "        '''\n",
    "        #h_theta = np.array(h_theta)\n",
    "        d = len(X)\n",
    "        h_theta = self.sigmoid(np.dot(X.reshape((1, d)), theta))\n",
    "        gradient = np.zeros((d,1))\n",
    "        # compute special case, the first column with x_i0 all being 1\n",
    "        gradient[0, 0] = h_theta - y # no regularization for the x_i0\n",
    "        # start compute the rest of columns\n",
    "        for j in range(d-1):\n",
    "            if self.regNorm is 1:\n",
    "                gradient[j+1, 0] = (h_theta - y) * X[j+1] + regLambda * ( theta[j+1]/ abs(theta[j+1] ) )\n",
    "            if self.regNorm is 2:\n",
    "                gradient[j+1, 0] = (h_theta - y) * X[j+1] + regLambda * theta[j+1]        \n",
    "        return gradient\n",
    "    \n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains the model\n",
    "        Arguments:\n",
    "            X is a n-by-d Pandas data frame\n",
    "            y is an n-by-1 Pandas data frame\n",
    "        Note:\n",
    "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
    "            Standardization should be optionally done before fit() is called.\n",
    "        '''\n",
    "        n = len(y)\n",
    "        X = X.to_numpy()\n",
    "        X = np.c_[np.ones((n, 1)), X]\n",
    "        \n",
    "        n, d = X.shape\n",
    "        y = y.to_numpy()\n",
    "        y = y.reshape(n, 1)\n",
    "        \n",
    "        # start doing gradient descent\n",
    "        if self.theta is None:\n",
    "            #self.theta = np.matrix(np.zeros((d,1)))  #np.zeros(d)\n",
    "            self.theta = np.matrix(np.random.rand(d, 1) - 0.5)\n",
    "        \n",
    "        \n",
    "        count = 0\n",
    "        G = np.zeros((d, 1))\n",
    "        self.alpha = np.ones((d,1)) * self.alpha\n",
    "        self.costList.append((None, 100)) # assign a large dummy theta and assign None to cost as the starter \n",
    "        allData = np.c_[X, y]\n",
    "        prev_theta = np.matrix(np.ones((d,1))) # initialize a dummy theta value\n",
    "        for rep in range(self.maxNumIters):\n",
    "            np.random.shuffle(allData)\n",
    "            X = allData[:,:-1]\n",
    "            y = allData[:,-1]\n",
    "            for i in range(n):\n",
    "                G = G + ((self.computeGradient(self.theta, X[i,:], y[i], self.regLambda)).reshape((d,1)))**2\n",
    "                alpha_t = self.alpha / (np.sqrt(G) + self.Xi) \n",
    "                prev_theta = self.theta # store the previous theta values\n",
    "                self.theta = self.theta - alpha_t * self.computeGradient(self.theta, X[i,:], y[i], self.regLambda)\n",
    "            count = count + 1\n",
    "            # print('Iteration ', count, ' theta: ', self.theta.T)\n",
    "            if self.hasConverged(self.theta, prev_theta):\n",
    "                break\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Used the model to predict values for each instance in X\n",
    "        Arguments:\n",
    "            X is a n-by-d Pandas data frame\n",
    "        Returns:\n",
    "            an n-by-1 dimensional Pandas data frame of the predictions\n",
    "        Note:\n",
    "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
    "            Standardization should be optionally done before predict() is called.\n",
    "        '''\n",
    "        prob = self.predict_proba(X)\n",
    "        label = pd.DataFrame((prob >= 0.5))\n",
    "        return label\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Used the model to predict the class probability for each instance in X\n",
    "        Arguments:\n",
    "            X is a n-by-d Pandas data frame\n",
    "        Returns:\n",
    "            an n-by-1 Pandas data frame of the class probabilities\n",
    "        Note:\n",
    "            Don't assume that X contains the x_i0 = 1 constant feature.\n",
    "            Standardization should be optionally done before predict_proba() is called.\n",
    "        '''\n",
    "        n, d = X.shape\n",
    "        X = X.to_numpy()\n",
    "        X = np.c_[np.ones((n, 1)), X]\n",
    "        prob = pd.DataFrame(self.sigmoid(np.dot(X, self.theta)))\n",
    "        return prob\n",
    "\n",
    "    def hasConverged(self, theta, prevTheta):\n",
    "        running_epsilon = np.linalg.norm( self.theta - prevTheta)\n",
    "        # print('The convergence now is: ', running_epsilon)\n",
    "        return (running_epsilon < self.epsilon)\n",
    "\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        '''\n",
    "        Computes the sigmoid function 1/(1+exp(-z))\n",
    "        '''\n",
    "        threshold_upper = 30\n",
    "        threshold_lower = -30\n",
    "        Z[Z > threshold_upper] = threshold_upper\n",
    "        Z[Z < threshold_lower] = threshold_lower\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getOptimalLambda_CV:\n",
    "    \n",
    "    def __init__(self, folds, repeats):\n",
    "        self.folds = folds\n",
    "        self.repeats = repeats\n",
    "    \n",
    "    def get_cvScore(self, logReg_model, X, y):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        logReg_model: a logistic regression model object\n",
    "        X: a n X d pandas.dataframe\n",
    "        y: a n X 1 pandas.series\n",
    "        \"\"\"\n",
    "        #logRegModel = logReg_model(self, regLambda, regNorm, epsilon, maxNumIters = 10000)\n",
    "        from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "        random_seed = 42\n",
    "        rskf = RepeatedStratifiedKFold(n_splits = self.folds, n_repeats = self.repeats,\n",
    "                                       random_state = random_seed)\n",
    "        scores = np.zeros(self.folds * self.repeats) # intialize a score array with 0 entries\n",
    "        times = np.zeros(self.folds * self.repeats)\n",
    "        # loop through all the trials(repetitions) and all the folds. \n",
    "        # Two for loops nested together in fact\n",
    "        # the dataset gets shuffled before each trial/repetition\n",
    "        count = 0\n",
    "        import time\n",
    "        for train_index, test_index in rskf.split(X, y):\n",
    "            start_time = time.time()\n",
    "            X_train, X_test = X.loc[train_index, :], X.loc[test_index, :]\n",
    "            y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "            y_test = pd.DataFrame(y_test)\n",
    "            logReg_model.reset_values() # reset the model initial parameters\n",
    "            logReg_model.fit(X_train, y_train)\n",
    "            y_predict = logReg_model.predict(X_test)\n",
    "            # calculate the accuracy\n",
    "            scores[count] = (y_test.to_numpy() == y_predict.to_numpy()).mean()\n",
    "            times[count] = time.time() - start_time\n",
    "            #print('---------------------------------------------------------------', scores[count])\n",
    "            count += 1 # update the counter\n",
    "        timeScore = times.mean()\n",
    "        cvScore = scores.mean() # get the mean accuracy\n",
    "        #print('The score matrix is: ', scores)\n",
    "        #print('The cv score is: ', cvScore)\n",
    "        #print('The time score is: ', timeScore, ' in seconds')\n",
    "        return cvScore\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************The logreg section:****************\n",
      "*************The adagrad section:****************\n",
      "--------------------------------------------------------------- 0.9824561403508771\n",
      "--------------------------------------------------------------- 0.9473684210526315\n",
      "--------------------------------------------------------------- 0.9736842105263158\n",
      "--------------------------------------------------------------- 0.9649122807017544\n",
      "--------------------------------------------------------------- 0.9911504424778761\n",
      "--------------------------------------------------------------- 0.9824561403508771\n",
      "--------------------------------------------------------------- 0.9824561403508771\n",
      "--------------------------------------------------------------- 0.9649122807017544\n",
      "--------------------------------------------------------------- 0.9649122807017544\n",
      "--------------------------------------------------------------- 0.9734513274336283\n",
      "--------------------------------------------------------------- 0.9298245614035088\n",
      "--------------------------------------------------------------- 0.9912280701754386\n",
      "--------------------------------------------------------------- 0.9824561403508771\n",
      "--------------------------------------------------------------- 0.9649122807017544\n",
      "--------------------------------------------------------------- 0.9380530973451328\n",
      "The score matrix is:  [0.98245614 0.94736842 0.97368421 0.96491228 0.99115044 0.98245614\n",
      " 0.98245614 0.96491228 0.96491228 0.97345133 0.92982456 0.99122807\n",
      " 0.98245614 0.96491228 0.9380531 ]\n",
      "The cv score is:  0.9689489209750038\n",
      "The time score is:  0.49504068692525227  in seconds\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for wdbc.csv\n",
    "# Load all data tables\n",
    "baseDir = \"\" ## TODO: insert path to data file\n",
    "df_wdbc = pd.read_csv(baseDir+'hw3-wdbc.csv', names = range(31)) # add columns names\n",
    "y = (df_wdbc.iloc[:, -1] == df_wdbc.iloc[0,-1]).astype('int32') # change the label to binary values\n",
    "df_wdbc.iloc[:, -1] = y\n",
    "# # Standardize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardizer = StandardScaler()\n",
    "X = df_wdbc.iloc[:,:-1]\n",
    "X = pd.DataFrame(standardizer.fit_transform(X))  # compute mean and stdev on training set for standardization\n",
    "df_wdbc.iloc[:,:-1] = X\n",
    "# # logistic regression with both L1 and L2 regularization\n",
    "folds = 5\n",
    "repeats = 3\n",
    "# logistic regression\n",
    "print('*************The logreg section:****************')\n",
    "logregModel = LogisticRegression(alpha = 0.05, regLambda = 1e-02, regNorm = 2, epsilon = 0.006, maxNumIters = 10000, initTheta = None)\n",
    "cvScoreCalculator = getOptimalLambda_CV(folds, repeats)\n",
    "X = df_wdbc.iloc[:, :-1]\n",
    "y = df_wdbc.iloc[:, -1]\n",
    "cvScores = cvScoreCalculator.get_cvScore(logregModel, X, y)\n",
    "# logistic regression with adagrad\n",
    "print('*************The adagrad section:****************')\n",
    "logregModelAdagrad = LogisticRegressionAdagrad(alpha = 0.05, regLambda = 1e-02, regNorm = 2, epsilon = 0.006, maxNumIters = 10000, initTheta = None)\n",
    "cvScoreCalculator_adagrad = getOptimalLambda_CV(folds, repeats)\n",
    "cvScores_adagrad = cvScoreCalculator_adagrad.get_cvScore(logregModelAdagrad, X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************The logreg section:****************\n",
      "--------------------------------------------------------------- 0.8008658008658008\n",
      "--------------------------------------------------------------- 0.6956521739130435\n",
      "--------------------------------------------------------------- 0.7347826086956522\n",
      "--------------------------------------------------------------- 0.7391304347826086\n",
      "--------------------------------------------------------------- 0.7347826086956522\n",
      "--------------------------------------------------------------- 0.6926406926406926\n",
      "--------------------------------------------------------------- 0.7043478260869566\n",
      "--------------------------------------------------------------- 0.7478260869565218\n",
      "--------------------------------------------------------------- 0.7869565217391304\n",
      "--------------------------------------------------------------- 0.7565217391304347\n",
      "--------------------------------------------------------------- 0.7316017316017316\n",
      "--------------------------------------------------------------- 0.7521739130434782\n",
      "--------------------------------------------------------------- 0.7391304347826086\n",
      "--------------------------------------------------------------- 0.7521739130434782\n",
      "--------------------------------------------------------------- 0.7130434782608696\n",
      "The score matrix is:  [0.8008658  0.69565217 0.73478261 0.73913043 0.73478261 0.69264069\n",
      " 0.70434783 0.74782609 0.78695652 0.75652174 0.73160173 0.75217391\n",
      " 0.73913043 0.75217391 0.71304348]\n",
      "The cv score is:  0.738775330949244\n",
      "The time score is:  7.068819904327393  in seconds\n",
      "*************The adagrad section:****************\n",
      "--------------------------------------------------------------- 0.47186147186147187\n",
      "--------------------------------------------------------------- 0.5782608695652174\n",
      "--------------------------------------------------------------- 0.5260869565217391\n",
      "--------------------------------------------------------------- 0.5521739130434783\n",
      "--------------------------------------------------------------- 0.5260869565217391\n",
      "--------------------------------------------------------------- 0.5844155844155844\n",
      "--------------------------------------------------------------- 0.5391304347826087\n",
      "--------------------------------------------------------------- 0.40869565217391307\n",
      "--------------------------------------------------------------- 0.4652173913043478\n",
      "--------------------------------------------------------------- 0.5695652173913044\n",
      "--------------------------------------------------------------- 0.5670995670995671\n",
      "--------------------------------------------------------------- 0.5826086956521739\n",
      "--------------------------------------------------------------- 0.6\n",
      "--------------------------------------------------------------- 0.46956521739130436\n",
      "--------------------------------------------------------------- 0.5565217391304348\n",
      "The score matrix is:  [0.47186147 0.57826087 0.52608696 0.55217391 0.52608696 0.58441558\n",
      " 0.53913043 0.40869565 0.46521739 0.56956522 0.56709957 0.5826087\n",
      " 0.6        0.46956522 0.55652174]\n",
      "The cv score is:  0.5331526444569923\n",
      "The time score is:  0.5992704709370931  in seconds\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for retinopathy.csv\n",
    "# Load all data tables\n",
    "baseDir = \"\" ## TODO: insert path to data file\n",
    "df_ret = pd.read_csv(baseDir+'hw3-retinopathy.csv', names = range(20)) # add columns names\n",
    "# # Standardize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardizer = StandardScaler()\n",
    "X = df_ret.iloc[:,:-1]\n",
    "X = pd.DataFrame(standardizer.fit_transform(X))  # compute mean and stdev on training set for standardization\n",
    "df_ret.iloc[:,:-1] = X\n",
    "# # logistic regression with both L1 and L2 regularization\n",
    "folds = 5\n",
    "repeats = 3\n",
    "X = df_ret.iloc[:, :-1]\n",
    "y = df_ret.iloc[:, -1]\n",
    "# logistic regression\n",
    "print('*************The logreg section:****************')\n",
    "logregModel = LogisticRegression(alpha = 0.001, regLambda = 1e-02, regNorm = 2, epsilon = 0.0006, maxNumIters = 10000, initTheta = None)\n",
    "cvScoreCalculator = getOptimalLambda_CV(folds, repeats)\n",
    "cvScores = cvScoreCalculator.get_cvScore(logregModel, X, y)\n",
    "# # logistic regression with adagrad\n",
    "print('*************The adagrad section:****************')\n",
    "logregModelAdagrad = LogisticRegressionAdagrad(alpha = 0.001, regLambda = 1e-02, regNorm = 2, epsilon = 0.0006, maxNumIters = 10000, initTheta = None)\n",
    "cvScoreCalculator_adagrad = getOptimalLambda_CV(folds, repeats)\n",
    "cvScores_adagrad = cvScoreCalculator_adagrad.get_cvScore(logregModelAdagrad, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************The logreg section:****************\n",
      "--------------------------------------------------------------- 0.7792207792207793\n",
      "--------------------------------------------------------------- 0.7857142857142857\n",
      "--------------------------------------------------------------- 0.7922077922077922\n",
      "--------------------------------------------------------------- 0.7581699346405228\n",
      "--------------------------------------------------------------- 0.7450980392156863\n",
      "--------------------------------------------------------------- 0.7142857142857143\n",
      "--------------------------------------------------------------- 0.7792207792207793\n",
      "--------------------------------------------------------------- 0.7987012987012987\n",
      "--------------------------------------------------------------- 0.7450980392156863\n",
      "--------------------------------------------------------------- 0.7647058823529411\n",
      "--------------------------------------------------------------- 0.7792207792207793\n",
      "--------------------------------------------------------------- 0.7467532467532467\n",
      "--------------------------------------------------------------- 0.7922077922077922\n",
      "--------------------------------------------------------------- 0.8366013071895425\n",
      "--------------------------------------------------------------- 0.7450980392156863\n",
      "The score matrix is:  [0.77922078 0.78571429 0.79220779 0.75816993 0.74509804 0.71428571\n",
      " 0.77922078 0.7987013  0.74509804 0.76470588 0.77922078 0.74675325\n",
      " 0.79220779 0.83660131 0.74509804]\n",
      "The cv score is:  0.7708202472908356\n",
      "The time score is:  0.07862294514973958  in seconds\n",
      "*************The adagrad section:****************\n",
      "--------------------------------------------------------------- 0.38961038961038963\n",
      "--------------------------------------------------------------- 0.38961038961038963\n",
      "--------------------------------------------------------------- 0.5714285714285714\n",
      "--------------------------------------------------------------- 0.32679738562091504\n",
      "--------------------------------------------------------------- 0.49673202614379086\n",
      "--------------------------------------------------------------- 0.4090909090909091\n",
      "--------------------------------------------------------------- 0.6818181818181818\n",
      "--------------------------------------------------------------- 0.37012987012987014\n",
      "--------------------------------------------------------------- 0.5555555555555556\n",
      "--------------------------------------------------------------- 0.5228758169934641\n",
      "--------------------------------------------------------------- 0.5974025974025974\n",
      "--------------------------------------------------------------- 0.6623376623376623\n",
      "--------------------------------------------------------------- 0.525974025974026\n",
      "--------------------------------------------------------------- 0.49673202614379086\n",
      "--------------------------------------------------------------- 0.5163398692810458\n",
      "The score matrix is:  [0.38961039 0.38961039 0.57142857 0.32679739 0.49673203 0.40909091\n",
      " 0.68181818 0.37012987 0.55555556 0.52287582 0.5974026  0.66233766\n",
      " 0.52597403 0.49673203 0.51633987]\n",
      "The cv score is:  0.5008290184760773\n",
      "The time score is:  0.3606383482615153  in seconds\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for retinopathy.csv\n",
    "# Load all data tables\n",
    "baseDir = \"\" ## TODO: insert path to data file\n",
    "df_dia = pd.read_csv(baseDir+'hw3-diabetes.csv', names = range(9)) # add columns names\n",
    "y = (df_dia.iloc[:, -1] == df_dia.iloc[0,-1]).astype('int32') # change the label to binary values\n",
    "df_dia.iloc[:, -1] = y\n",
    "# # Standardize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardizer = StandardScaler()\n",
    "X = df_dia.iloc[:,:-1]\n",
    "X = pd.DataFrame(standardizer.fit_transform(X))  # compute mean and stdev on training set for standardization\n",
    "df_dia.iloc[:,:-1] = X\n",
    "# # logistic regression with both L1 and L2 regularization\n",
    "folds = 5\n",
    "repeats = 3\n",
    "X = df_dia.iloc[:, :-1]\n",
    "y = df_dia.iloc[:, -1]\n",
    "# logistic regression\n",
    "print('*************The logreg section:****************')\n",
    "logregModel = LogisticRegression(alpha = 0.001, regLambda = 1e-02, regNorm = 1, epsilon = 0.0001, maxNumIters = 10000, initTheta = None)\n",
    "cvScoreCalculator = getOptimalLambda_CV(folds, repeats)\n",
    "cvScores = cvScoreCalculator.get_cvScore(logregModel, X, y)\n",
    "# logistic regression with adagrad\n",
    "print('*************The adagrad section:****************')\n",
    "logregModelAdagrad = LogisticRegressionAdagrad(alpha = 0.001, regLambda = 1e-02, regNorm = 1, epsilon = 0.0001, maxNumIters = 10000, initTheta = None)\n",
    "cvScoreCalculator_adagrad = getOptimalLambda_CV(folds, repeats)\n",
    "cvScores_adagrad = cvScoreCalculator_adagrad.get_cvScore(logregModelAdagrad, X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.55835372 0.         0.         0.         0.         0.        ]]\n",
      "*****************************\n",
      "[[0.55835372 0.58670556 0.         0.         0.         0.        ]]\n",
      "*****************************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-8e7155484ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mlogregModel_L2_ada\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionAdagrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregLambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregNorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxNumIters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitTheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mcvScoreCalculator_L2_ada\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetOptimalLambda_CV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mcvScores_L2_ada\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvScoreCalculator_L2_ada\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cvScore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogregModel_L2_ada\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0maccuracyList_L2_ada\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvScores_L2_ada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracyList_L2_ada\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-19748e0b4ff2>\u001b[0m in \u001b[0;36mget_cvScore\u001b[0;34m(self, logReg_model, X, y)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mlogReg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reset the model initial parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mlogReg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogReg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# calculate the accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-2f3ec729cd62>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregLambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m                 \u001b[0malpha_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mprev_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;31m# store the previous theta values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Preprocessing for wdbc.csv\n",
    "# Load all data tables\n",
    "baseDir = \"\" ## TODO: insert path to data file\n",
    "df_ret = pd.read_csv(baseDir+'hw3-retinopathy.csv', names = range(20)) # add columns names\n",
    "# # Standardize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardizer = StandardScaler()\n",
    "X = df_ret.iloc[:,:-1]\n",
    "X = pd.DataFrame(standardizer.fit_transform(X))  # compute mean and stdev on training set for standardization\n",
    "df_ret.iloc[:,:-1] = X\n",
    "# # logistic regression with both L1 and L2 regularization\n",
    "folds = 5\n",
    "repeats = 3\n",
    "X = df_ret.iloc[:, :-1]\n",
    "y = df_ret.iloc[:, -1]\n",
    "# Building Learning Curve\n",
    "#maxInter = [1,5,10,20,30,40,50,60,70,80,90,100,150,200,250,270,290,310,500,1000,1500,2000,2500]\n",
    "maxInter = [1,5,10,20,30,300]\n",
    "accuracyList_L2 = np.zeros((1, len(maxInter)))\n",
    "accuracyList_L1 = np.zeros((1, len(maxInter)))\n",
    "accuracyList_L2_ada = np.zeros((1, len(maxInter)))\n",
    "accuracyList_L1_ada = np.zeros((1, len(maxInter)))\n",
    "\n",
    "for index, inter in enumerate(maxInter):\n",
    "    # logreg L2\n",
    "#     logregModel_L2 = LogisticRegression(alpha = 0.01, regLambda = 1e-08, regNorm = 2, epsilon = 1e-20, maxNumIters = inter, initTheta = None)\n",
    "#     cvScoreCalculator_L2 = getOptimalLambda_CV(folds, repeats)\n",
    "#     cvScores_L2 = cvScoreCalculator_L2.get_cvScore(logregModel_L2, X, y)\n",
    "#     accuracyList_L2[0,index] = cvScores_L2\n",
    "#     print(accuracyList_L2)\n",
    "#     print('*****************************')\n",
    "    # logreg L1\n",
    "#     logregModel_L1 = LogisticRegression(alpha = 0.01, regLambda = 1e-08, regNorm = 1, epsilon = 1e-20, maxNumIters = inter, initTheta = None)\n",
    "#     cvScoreCalculator_L1 = getOptimalLambda_CV(folds, repeats)\n",
    "#     cvScores_L1 = cvScoreCalculator_L1.get_cvScore(logregModel_L1, X, y)\n",
    "#     accuracyList_L1[0,index] = cvScores_L1\n",
    "#     print(accuracyList_L1)\n",
    "#     print('*****************************')\n",
    "    # adagrad L2\n",
    "    logregModel_L2_ada = LogisticRegressionAdagrad(alpha = 0.01, regLambda = 1e-08, regNorm = 2, epsilon = 1e-20, maxNumIters = inter, initTheta = None)\n",
    "    cvScoreCalculator_L2_ada = getOptimalLambda_CV(folds, repeats)\n",
    "    cvScores_L2_ada = cvScoreCalculator_L2_ada.get_cvScore(logregModel_L2_ada, X, y)\n",
    "    accuracyList_L2_ada[0,index] = cvScores_L2_ada\n",
    "    print(accuracyList_L2_ada)\n",
    "    print('*****************************')\n",
    "    # adagrad L1\n",
    "#     logregModel_L1_ada = LogisticRegressionAdagrad(alpha = 0.01, regLambda = 1e-08, regNorm = 1, epsilon = 1e-20, maxNumIters = inter, initTheta = None)\n",
    "#     cvScoreCalculator_L1_ada = getOptimalLambda_CV(folds, repeats)\n",
    "#     cvScores_L1_ada = cvScoreCalculator_L1_ada.get_cvScore(logregModel_L1_ada, X, y)\n",
    "#     accuracyList_L1_ada[0,index] = cvScores_L1_ada\n",
    "#     print(accuracyList_L1_ada)\n",
    "#     print('*****************************')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD5CAYAAAAp8/5SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXxU1f3/8deHrCUsCRB2MKiIILsRVxQXNm1FSlXcrVX7Vflqv/4sSq3Uunzb2mrVSt1xV0QBxX6xgKJSFZUoOwiEuAVQIquAWSb5/P44d5LJZJskk0xy83k+HvcxM3fuzZyTSd5z5txzzxVVxRhjjH+1inUBjDHGNCwLemOM8TkLemOM8TkLemOM8TkLemOM8TkLemOM8bn4SDYSkXHAA0Ac8ISq/rmSbc4DbgcUWKWqF3rr7wHOwn2oLAZu0GrGdHbq1EkzMjJqVwtjjGnhPv300+9VNb2y52oMehGJA2YAo4FcYLmIzFfV9SHb9AWmASeq6m4R6eytPwE4ERjsbfo+cArwblWvl5GRQVZWViT1MsYY4xGRr6p6LpKumxFAtqrmqGohMAuYELbNVcAMVd0NoKo7vPUKJAOJQBKQAHxXu+IbY4ypj0iCvgfwTcjjXG9dqCOAI0TkAxH5yOvqQVWXAe8A271loapuCH8BEblaRLJEJCsvL68u9TDGGFOFaB2MjQf6AqOAC4DHRSRVRA4H+gM9cR8Op4nIyPCdVfUxVc1U1cz09Eq7mIwxxtRRJAdjtwK9Qh739NaFygU+VtUi4AsR2URZ8H+kqvsBRORN4HjgP7UpZFFREbm5ueTn59dmNxMmOTmZnj17kpCQEOuiGGMaUSRBvxzoKyJ9cAE/GbgwbJvXcC35p0SkE64rJwc4FLhKRP4ECO5A7P21LWRubi5t27YlIyMDEant7gZQVXbu3Elubi59+vSJdXGMMY2oxq4bVQ0AU4CFwAZgtqquE5E7RORsb7OFwE4RWY/rk/+tqu4EXgW2AGuAVbhhl2/UtpD5+fl07NjRQr4eRISOHTvatyJjWqCIxtGr6gJgQdi66SH3FbjRW0K3KQZ+Xf9iYiEfBfY7NKZliijojTGxcfAgLFoEa9ZA27bQvj2kprrb8CUxMdalNU2VBX2E2rRpw/79+2NdDKDystx333088cQTxMfHk56ezsyZMznkkENiVEJTa4EAbN8OW7eyZ9MO/rU4iXmfdOfNLUfwY3FSRD8iObnyD4DgUtUHhH1Y+J8FfSMrLi4mLi4u6j932LBhZGVl0bp1ax5++GGmTp3Kyy+/HPXXMXWwbx9s3Vr1kpvLt9/Ca0xgHhNZwngCJNCdrfyy7Wwm9v6UkwLvcnDj1+xNSGfvyJ+y99Rz2Nv/OPYeTGDvXqpctm4tu3/gQM1FrezDIpIPCPuwaNos6GtJVZk6dSpvvvkmIsLvf/97zj//fEpKSpgyZQpLliyhV69eJCQkcMUVV/CLX/yCjIwMzj//fBYvXszUqVM55phjuO6668jLy6N169Y8/vjjHHnkkWzZsoWLLrqIAwcOMGHCBO6///6Iv0WceuqppfePO+44nn/++Yb6FZig4mL47rsKoV0hyCt7D9PSyEk/lnlyFXNLTmUZh6G04vBuB7hxzC4mnpfAiDHdaBV/CXAJqJK8YgUdnnsOXnweltwHHTvC+efDpZfCiBFQwzGYQMB95lT3wRBc9uyp/4dFbT8g7MOi4TS/oP/Nb2Dlyuj+zKFD4f7IRn3OnTuXlStXsmrVKr7//nuOOeYYTj75ZD744AO+/PJL1q9fz44dO+jfvz9XXHFF6X4dO3bks88+A+D000/nkUceoW/fvnz88cdce+21LFmyhBtuuIEbbriBCy64gEceeaTO1XnyyScZP358nfc3uHCurhW+dSt8+60L+1Dx8dCtG/ToAQMHwtix7n7Pnmj3Hqw90Ie5y7oy71+JrFrldhk6FP44BSZOhKOOSkEkpWJ5RGD4cLf89a+u4/6552DmTPjnP+GII+CSS+Dii6GKSQHj46FDB7fUVU0fFqEfEKFLbm7tPix+8hPo25fS31GLUVICraI/qXDzC/oYe//997nggguIi4ujS5cunHLKKSxfvpz333+fc889l1atWtG1a9dyLWyA888/H4D9+/fz4Ycfcu6555Y+V1BQAMCyZct47bXXALjwwgu56aabal2+559/nqysLN577726VtHfSkpgx46aQ3zv3or7tmsHPXu64B4wwN2GL507l/tHLSmBjz+GefNg7nTYssVl9oknwr33unCv9WkN8fFw5plu2bsX5syBZ5+F225zy8iRrpV/7rmueRxFDflhEf4hkZwcvXI3WaqwcSO88YZbOnQALwOiqfkFfYQt76YmJcW10kpKSkhNTWVltL+VAG+99RZ333037733HklJkR3A85Uff6yxL5zt213ShGrVqqwV3q8fnHZa5SHepk1ExSgqgnffdeH+2mvuJRMS4PTTYepUmDABunSJUp3bt4crrnDLV1/BCy+40L/qKpgyxb3YJZe4bxZN5IzoaHxYNGtFRfD++2Xhnp3t1g8Z4t6nhqCqTWo5+uijNdz69esrrGtsKSkpqqo6Z84cHTNmjAYCAd2xY4f27t1bt2/frrNnz9azzjpLi4uL9dtvv9W0tDR95ZVXVFX1kEMO0by8vNKfdfzxx+vs2bNVVbWkpERXrlypqqpnnnmmzpo1S1VVH3300dLXrKosoT777DM99NBDddOmTdXWoyn8Lutl507Vxx9XnT5d9YorVMeOVR04UDUtTdW1j8ovbdqo9uunetppqpdconrLLar/+IfqvHmqn3yiunWraiBQ72IdOOB+5CWXqKamupdu3Vp10iTVF15Q3b07CnWPVEmJq9uUKaqdOrnCpKerXn+96vLl7nnTuHbuVH3+edXJk1Xbt3fvSWKi6rhxqjNmqH71Vb1fAsjSKnK1+bXoY2zixIksW7aMIUOGICLcc889dO3alUmTJvH2228zYMAAevXqxfDhw2lfxdfmF154gWuuuYa77rqLoqIiJk+ezJAhQ7j//vu5+OKLufvuuxk3blyV+x88eJCePXuWPr7xxhtZsGAB+/fvL+0S6t27N/Pnz4/+LyCW5s2Da65xB0BFoGtX19I+7DA4+eTKW+Ht2jVYcfbsgX/9yxXrzTfdF4q0NNeInjgRxoxxfc2NTgSOOcYt994L//63689/5BF48EHo39+18i+6CHr3jkEBW4jQLpkPPnDHczp3hkmT4Kc/hdGjI/6WWG9VfQLEammqLfpI/PDDD6qq+v333+uhhx6q27dvr9X+Bw4c0BKvtfXSSy/p2WefHfUyNpffZTk7dqied55rBQ0dqvrRR6qFhTEpyrZtqg8/rDpmjGp8vCtS9+6q112n+tZbMStWZHbtUn30UdWTTnIFF1E99VTVp55S3bcv1qVr/goLVZcsUb3xRtW+fcu+VQ4erHrrre7vtri4wV6ealr0MQ/28KU5B/0pp5yiQ4YM0f79++tTTz1V6/2XLl2qgwcP1kGDBunIkSN18+bNUS9jc/ldqqrrYpg1y3U/JCSo3nlnTJJ0yxbVv/1N9YQTXDaC+z+eOrXB/3cbzpYtqn/8o+phh7kK/eQnqhdcoPrmm6pFRbEuXfOxa5frm5s8uazPLjHRdSk+9JDql182WlEs6E2pZvO73L5ddeJE9yeamam6Zk2jvXRJieqqVaq33646ZEhZw2zYMNU77lBdu9ZH3dwlJaoffqh6zTVlxzm6dHGt0hUrfFTRKNq40X3yn3KKalyc+5117qz6y1+qzp2r6n2zb2wW9KZUk/9dlpSoPvusC52kJNW//KVRWpjFxS7vbrqprJEr4no57rtPNSenwYsQe/n5LqjOOcd9gwJ3oPuee1Rzc2NdutgpLFR9552KXTKDBqn+7neqy5Y1ia91FvSmVJP+Xebmqv70p+7P8vjjVTdsaNCXKyxUXbTINWa7dXMvm5DgBkI8+qjqt9826Ms3bd9/70aDHHdc2afe6NHuQzhGLdZGtWuX6osvuu6sGHfJRKq6oLdRNyb2VN0Znjfe6MYY//3v8N//DQ0wJ9DBg7BwoRsp88YbbuRM69bu3KOJE+Gss6J+jlHz1LEjXHutWzZvdqN2nnvOnYiVkgI//7kbuXPaaQ3yPsXEpk3uj+Jf/4L//MeNkklPh3POgZ/9zI2Sads21qWsEwt6E1tffQVXX+1O6T/5ZHjySTj88Ki+RHAY5Ny5bqThjz+6k3XOOceF++jRMRoG2Vz07Qt33AG33+6GCT73HMye7W67d3fDNC+91E350JwEAq4+wSGQmza59YMGwc03uyGQI0b444OsqqZ+6AKMAzYC2cAtVWxzHrAeWAe8GLK+N7AId3Wq9UBGda/VVLtuqjp5KRYqK8t7772nw4YN07i4uNITtSrTFH6Xqur6NP/5T3dCU0qK6yaIYj9nZcMge/RwwyDfftsGltTbjz+qzp6t+rOflf2Chw5VvfdedyC9qQp2yVx4YdnB54QE94fyj3+ofvFFrEtYZ9Snjx6Iw10O8FAgEXdJwAFh2/QFVgBp3uPOIc+9C4z27rcBWlf3en4P+kAUzsKsrCxffPGFrlq1Si+55JKmH/Rbtrjx26B6xhlR++fKzlb9618rDoO8+eZmPAyyOdixQ/XBB93oKFBt1cod6HjxRXfKcKxt2uQ+gEaNKhsl06mT6mWXqb76qm/OIahv0B8PLAx5PA2YFrbNPcCVlew7AHi/ptcIXZp60JeUlOhNN92kRx11lA4cOLB0yoLi4mK95pprtF+/fnrGGWfo+PHjy02BMHXqVB02bJi+9NJLmp2drWPHjtXhw4frSSedpBu8g47Z2dl67LHH6sCBA/XWW2+t1RQIQZdddlnTDfriYtUHHnBzA7Rr56YyqMfwvdBhkIMHa7lhkHfe6bNhkM3Fhg1uJEqvXu7NaNvWDTtcsqTxPmmLilTfe88NoerXr+wPY+BA1WnT3PCqKDS4mprqgj6SPvoewDchj3OBY8O2OQJARD7wvgHcrqr/9tbvEZG5QB/gLa/rp9zcriJyNXA1uFP3qxPjWYqbxTTFTdKmTW7irQ8+gPHj4dFHoVevWv+Y4GyQc+e6A6rB2SBPOgnuu8/1uVcxS69pDEceCXffDXfeCUuXugnWXn0VnnrKvd8XX+wO4vbvH93X3bPHHYB54w03H8Xu3W4St1Gj4Lrr3MHUFvyHEa2DsfG47ptRQE9gqYgM8taPBIYBXwMvA5cDT4burKqPAY8BZGZmapTK1CCa+jTFTU5xsRtFc9ttbt7Zp592B+5qcaHyQMBlxpw5LtxDZ4O8+WY4++wozgZpoqNVKxeyo0bBQw/B/Pku9O+5B/70J8jMdIE/ebKb/6UusrPLDqT+5z/uD6VTJ/cHERwl04BzHTUnkQT9ViC06dXTWxcqF/hYVYuAL0RkEy74c4GVqpoDICKvAccRFvS10UxnKW6UaYqbnPXr4Ze/hE8+cTN9Pfywmw44AgUF8PbbLtxffx127nTDIMePdyP7bBhkM9K6tQv0yZPdxVpeesmN2LnhBvh//w/GjXMf/j/7WfWT0AcC8OGHZUMgP//crT/qKLjpJrf/scf6Y5RMlEVyKZPlQF8R6SMiicBkIHxaxNdwrXlEpBOuyybH2zdVRNK97U7DjbxptkaOHMnLL79McXExeXl5LF26lBEjRnDiiScyZ84cSkpK+O6773j33Xcr3b9du3b06dOHV155BXDHSFZ5l9E57rjjmDNnDgCzZs1qlPo0iKIi+N//hWHDXN/Kiy+6pngNIX/ggAv2iy5yjbyzznLf+seNc101eXnu8YUXWsg3W127wv/8D3z2GaxZ486d+OwzOO8899xVV7nWeUmJ237vXnj5Zdfl06ULnHIKPPCA6wZ68EHIyYG1a923hBNOsJCvSlWd96ELcCawCTf65lZv3R3A2d59Ae7DhfgaYHLIvqOB1d76p4HE6l6rOR+M/fWvf116MPb000/XRYsWqWrF+ehzcnJ07NixOnjwYO3fv7/+8Y9/VFXVTZs26YgRI3TQoEH629/+Vrt3715pWUREe/ToUbrce++9+sknn2iPHj20devW2qFDBx0wYECl+zb473LVKtXhw93Br3PPVf3uu2o337PHTdM9caKbVys4IOLKK1UXLFAtKGjY4pomIBBQXbxY9dJL3VBbUM3IcKNkgkM3O3Z0z7/yiurevbEucZOETYHQOFr0NMUFBap/+IP7x+zc2Q1bq8KOHapPPKE6fnzZlCrdu7vrZCxZYmPcW7T9+1Wfe86Nax8yxI2Nff99X46Sibbqgt7OjI2in/70p+zZs4fCwkJuu+02unbtWqv9P/30U6ZMmYKqkpqaysyZMxuopFH26aduRM3q1a7f5YEH3Cn0IbZudZfVmzMH3nvPfTPv08d1006a5E5AbIBrIpvmJiXFddNcfHGsS+IrFvRRVFW/fKRGjhxZ2l/fLOTnu1Pj77nHdaq//rob8eDJyXF963PnwrJlbt2AAfC737lwHzKkVoNvjDF11GyCXlURS4V6cd/uouTjj92Img0b3O1990FqKuvXu2CfM6fsfIfhw93Q6p//3A2zNsY0rmYR9MnJyezcuZOOHTta2NeRqrJz506Sqxu+Fokff3Rj4v/+d+jeHV3wJiu6jGPOX13AB0e8nXCCu1zpxImui8YYEzvNIuh79uxJbm4ueXl5sS5Ks5acnFzuouK19v77cMUVlGzO5qMJf2ZOrxuYe20SX37pRrWNGuVmFz7nHDepoTGmaWgWQZ+QkEAfaxbGzoEDBG75Pe89tIa5bW9jXsfz2P56EomJ7uTD225zXfOdOsW6oMaYyjSLoDexUVAAb923mrn/+zmv77+VnXSidbEyfpQwaZK7WIeduGRM02dBb8o5cMDNDTVnVhH/ej3AD0WDadeqDz8b/SOTroGxY4XWrWNdSmNMbVjQN7C33nLTeRx/PIwd66681tRawXv3uqlD5swpuwJTp1b7OL9kHj+fqJw+8yISU+s48ZQxJuYs6BvY/Plubq+cHDczb1wcHHecC/2xY+Hoo2MzPUdenhv2Pneu+zAqKoLu3Ur4VcY7/HzDXYzsm0f800+4whpjmjU7F7GBrV3rwnzXLndG6M03u/OMpk93E+117uwm9Zs505092pC2bnUzxp52Wtn8UZ9/7s5OXfbX9/mG3vxj01hOveU44ldmWcgb4xMS1ZNooiAzM1OzsrJiXYyo6dLFXWP4ybCJmfPyXEt64UK3fPutW3/UUWWt/ZEj63/R6uDZqXPmwEcfuXUDBriTlyZNgiG9diG/uQGef95d3Pmpp9xc4caYZkVEPlXVSv95reumAe3Y4ZaBAys+l54OF1zgFlU3Y2sw9B96yJ1ompzsZmUdM8YF/4ABkU0ZsH69C/a5c2s4O3XePBh3jZvsffp0uPVWSEyMWv2NMU2Dtegb0DvvuG6SRYvcePNIHTzounmCwR8827Rnz7LQP+MM6NDBrVeFFSvKwj307NRJkyo5OzUvD6ZMgdmz3ZzxM2e66ykaY5ota9HHyLp17vaoo2q3X/BKSuPHu8dff+0+LBYudEE+c6ab6fGYY2DQINcFFNHZqaou3KdMgX374K67YOpUd10+Y4xvWdA3oLVrIS0t4qvnVal3b7jySrcEArB8uQv9RYvcVdlGjYrg7NRvv4Vrr3XdNccc4/ria/sJZIxpliIKehEZBzwAxAFPqOqfK9nmPOB2QIFVqnphyHPtcFefek1Vp0Sh3LHx9NPQo0fE/TDr1rn++WjOwxYf78bkH3883H57BDuougOtN9zg+oTuucddyi3ePuONaSlq/G8XkThgBu6SgLnAchGZr6rrQ7bpC0wDTlTV3SISfnbNncDS6BU7BlTd9S379Yso6FVdi37y5EYoW1Vyc+G//gv+7/9ch/3Mma78xpgWJZJx9COAbFXNUdVCYBYwIWybq4AZqrobQFV3BJ8QkaOBLsCi6BQ5RrZtg927Xb/J/v0Rbb5nT+UjbhqcqhvPedRRsGQJ3H8/LF1qIW9MCxVJ0PcAvgl5nOutC3UEcISIfCAiH3ldPYhIK+Be4KbqXkBErhaRLBHJarJTEa9e7W6Li+GDD2rcvK4HYuvtq6/csJwrr3Qjatascd02sTj91hjTJETrzNh4oC8wCrgAeFxEUoFrgQWqmlvdzqr6mKpmqmpmenp6lIoUZWvWuNu4OIjgkoFr17rbRgv6khJ4+GH3FeLDD2HGDNeaP+ywRiqAMaapiuSI3FagV8jjnt66ULnAx6paBHwhIptwwX88MFJErgXaAIkisl9Vb6l/0RvZmjVuIPshh0QU9OvWubNiG+Vza8sW14J/9113/OCxxyAjoxFe2BjTHETSol8O9BWRPiKSCEwG5odt8xquNY+IdMJ15eSo6kWq2ltVM3DdN8/GPOS/+gr++c/a77dmjRu0PmpUlf30eXlw3XVuDpl//7uBW/OFhW5+g7//HQYPhs8+g8cfd+MuLeSNMSFqbNGrakBEpgALccMrZ6rqOhG5A8hS1fnec2NEZD1QDPxWVXc2ZMHr7PHH3VwAF18M7dpFtk9RkZtXYOxYF/R33+366ceOLbfZ3LnuM6RrV/d40qR6lLOgAL75xp0JFVy++qrs/rZtrrsG3BVAHn3UfeMwxpgwEQ2mVtUFwIKwddND7itwo7dU9TOeBp6uSyGjassWd3vgQORBv2mTC/vBg90A9oQE100SFvQrVkBqqsvgGsfO5+e7U16rCvLt293omaC4OBfkGRlw+unuNiMDjjjClckumm6MqULLO2smJ8fdVjNEMhBwuVqancEDsYMGQUoKjBhRaT/9ypVuyphymbtvH8ya5V43NMiD01UGxcdDr14uvMeOdbeHHFIW6D162ElOxpg6aXnJEdqir4Sq61sfPdrNIgm4oZXx8WXTPo4aBX/+s/uwaNMGcKMuV6925yeVKihw3SoffOC+BfTu7UL7rLPKh3hGhpuYxoZAGmMaQMsK+r173ZS8UGXQf/ed66nZtMlN6XvaabgW/ZFHlk3hW0k//aZN7hJ8Qze8BMsy3EU7fvUrt80LL8D551uQG2NiomVdYSrYbQNVdt0Ee2lat3ajZw4epGzETVBoP70nOO/70H//yU03cOSRLuDvugsuvNBC3hgTMy0r6IPdNlBliz4Y9M884z4Xpt9c4PrWQ4O+kn76FSsgsVUR/dttgwcecH05V18Nv/tdA1TEGGMiZ0EfZs0ad6LTL34Bv/41/P2fiXzCMeWDHiqMp1+5Egb+ZAsJQwbA9ddDdrYb8mijYYwxMdaygj4np6wLpZqum2Cm/+Uv0K3dAX7FkxQeObj8hqNGlc57467wpAwr+AiGDGm48htjTB34N+g3b4ZXXim/7rvv3MgXqLRFX1zszosKBn379vDwCc+xlkH8+YVe5TcO6afftg2+/14YGsiyoDfGNDn+DfpHHoErrii/Lj8fOnZ09ysJ+pwcN3ImdGrhn/3wEhd0Wsxdd0vpjJRAuX76FSvcqmGscCdVGWNME+LfoC8ocEuo/Hw3nCY5udKgDz0vCnCD6lev5oGzFtGunRstWVwcsoPXT7/yE/c6g2VtjCagN8aYqvl3HH1RkVtUyw6IFhS4aQ/atKm0j37NGrdp6WRkubmwdy/pxx7Kg2PgoovglFNcl44qlOT9Bi0eydp/KoenbKNtz27ug8QYY5oQfwc9uCZ4cOqA/Hzo3Nl1u1TRoj/ssJCsDmniX3CimyByyRL3Y1q1AiGNVtKeQ5K/4wJ9yvrnjTFNkv+DvrCwLOgLCiApia2JfRj72p0s+Lrs2CxUPC+q9KpSAwciAn/7W/iLxMFJN8GuXbB1Awy5u4EqY4wxdeffPvpg0AdvwTXFk5NZK4NYt693aY6DOwibnR0W9GvWuE+C1NSqX2fUKNiwwd23A7HGmCbIv0EfCLjb8KBPSuJAYhoAP/xQ9tSGDW5693LHUis08SsxalTZfeu6McY0QREFvYiME5GNIpItIpVeIUpEzhOR9SKyTkRe9NYNFZFl3rrVInJ+NAtfrcpa9AUFkJzMwXg3D31o0FcYcVNYCJ9/XnPQB8fTp6XZhT+MMU1SjX30IhIHzABG464Nu1xE5qvq+pBt+gLTgBNVdbeIdPaeOghcqqqbRaQ78KmILFTVPVGvSbjQPvqgYIs+vj1QMeiTkuDww70VGze6n1FT0KekwKmnup1tugNjTBMUycHYEUC2quYAiMgsYAKwPmSbq4AZqrobQFV3eLebghuo6jYR2QGkA40X9JX00R9o1RaoGPQDBoRc2yPYxI+k333uXAt5Y0yTFUnXTQ/gm5DHud66UEcAR4jIByLykYiMC/8hIjICSAS2hD/XIMKDPhBwnfDJyRxo5S4WEhr0a9dWciA2IQH69av5tVJSbPy8MabJitbwynigLzAK6AksFZFBwS4aEekGPAdcpqol4TuLyNXA1QC9Q8c71kf4wdj8fHeblMQBXNDv26uAsGuXu85rhQOxRx7pwt4YY5qxSFr0W4HQGb16eutC5QLzVbVIVb8ANuGCHxFpB/wfcKuqflTZC6jqY6qaqaqZ6enpta1D5cJb9MHpEJKTOSiu9f3DPveZU+FALLgx9DX1zxtjTDMQSdAvB/qKSB8RSQQmA/PDtnkN15pHRDrhunJyvO3nAc+q6qtRK3Ukwg/G5uezkw5sPZjGgRIv6Pe4iWsqBP2ePfDNNzYu3hjjCzUGvaoGgCnAQmADMFtV14nIHSJytrfZQmCniKwH3gF+q6o7gfOAk4HLRWSltwxtkJqEC2/R5+dzI/cx6fGxHChJBuCHvWUt+rQ0d31uwHXYg7XojTG+EFEfvaouABaErZsecl+BG70ldJvngefrX8w6CAb8jz+66YrPOYc80sndnUJ6e9fK/2Gf2yR4XlTpwJngKbMW9MYYH/DvmbHBoN+yBZ56Ct54g3yS2XMwkQNFSQD8kPcjuvQ/rF2rFUfcpKbaCVDGGF/w76RmwVE3wemI8/LIJ5kD+fHsyXdBv29nEV+fcjE/8BUDX/49bN8AQ4fC0qVhTXxjjGm+/N+iDwb999+Tj+ub37bPG3IOpWEAABJbSURBVEef2JE1f5gDwKBB4lry06e76wlmZjZ6kY0xpiH4t0UfDPrgWVEhQb9jh2up5xfGsSLOBfrAeXdC+zvd9p9/7sbQG2OMD7ScoPe6bsBdHSpo2TI3E3H79t6Ktm3hmGMar5zGGNPAWk7Xze7dpUEPZVPML1tmg2uMMf7m36APBFDgix0p7rFquaDv2tXd7tljQW+M8Tf/Bn1REYsYw2HvPM6XHAJQadBD2Bw3xhjjM/4M+uJiUOUrDkFpxTf0Qqk66K1Fb4zxM38Gvdc/vxd3hHU3aRSRgIZUNxj08fE2wMYY428tJuhDW/NQFvT9+kFiYqOWzhhjGpWvg34PbmhNdUFv3TbGGL/zZ9B70x9U16Lv1s0NsTzhhEYvnTHGNCp/njAVQddNaips3uymJzbGGD/zZ4s+LOh30aE06INXBkxJgU6dIC4uJiU0xphG0yKCPrRFH+ybT0mJScmMMabRRRT0IjJORDaKSLaI3FLFNueJyHoRWSciL4asv0xENnvLZdEqeLWqORhrQW+MaWlq7KMXkThgBjAadxHw5SIyX1XXh2zTF5gGnKiqu0Wks7e+A/AHIBNQ4FNv393Rr0qIalr0/frBunVu7jJjjGkJImnRjwCyVTVHVQuBWcCEsG2uAmYEA1xVd3jrxwKLVXWX99xiYFx0il4Nb56bfbQDygf9dde5oE9OrmZ/Y4zxkUiCvgfwTcjjXG9dqCOAI0TkAxH5SETG1WJfRORqEckSkay8vLzIS1+VoiL204YS4kigkN2k8SM/AaBdO8jIqP9LGGNMcxGtg7HxQF9gFHAB8LiIpEa6s6o+pqqZqpqZnp5e/9IUFZX2z/fmawpJYjduHKW15I0xLU0kQb8V6BXyuKe3LlQuMF9Vi1T1C2ATLvgj2Tf6iopK++cz+BKA7XQDLOiNMS1PJEG/HOgrIn1EJBGYDMwP2+Y1XGseEemE68rJARYCY0QkTUTSgDHeuoYVEvR9+AKAbV6PkQW9MaalqXHUjaoGRGQKLqDjgJmquk5E7gCyVHU+ZYG+HigGfquqOwFE5E7chwXAHaq6qyEqUk4gUCHot8f3hIAFvTGm5YloCgRVXQAsCFs3PeS+Ajd6S/i+M4GZ9StmLVXWdZPcB/ZDUlKjlsQYY2LOt2fGBg/GBoN+W5t+JCTYlAfGmJbHt0EfbNH3YyMA335r3TbGmJbJ10GfIEV0YBepiQcAC3pjTMvk26DfQyqp8fsRoHvKXsCC3hjTMvkz6AMBdpNGaoJryXdv8wNgQW+MaZn8GfReiz4t6SAAPdq7oLcRN8aYlsh/Qb9tG+zf71r0yfkAdG/vAt9a9MaYlsh/lxLs4c6A3cPn9PnJPreqw4+ABb0xpmXyV4tetfTubtJITXHz0nfv4Fr2FvTGmJbIX0FfXAy4K5zsIZW0NoUAdO/kbi3ojTEtkb+CPhAA4CCtKSKR1NauRd+js7u1oDfGtES+DPrg9AdpbVzAd+lUjIgFvTGmZfJX0HtdN8GLjAT76BNSEjn0UOjcOWYlM8aYmPHXqJvwFn1b95jkZN57z11G0BhjWhpfBn1pi76ta+GTlBQcdWmMMS2OL7tuSlv07Uvcejsl1hjTgkUU9CIyTkQ2iki2iNxSyfOXi0ieiKz0litDnrtHRNaJyAYReVBEJJoVKCe8Rd/Ogt4YY2rsuhGROGAGMBp3EfDlIjJfVdeHbfqyqk4J2/cE4ERgsLfqfeAU4N16lrtyYX30qe29E6gs6I0xLVgkLfoRQLaq5qhqITALmBDhz1cgGUgEkoAE4Lu6FDQiIS36Nq0OEN/RXXyE1NQGe0ljjGnqIgn6HsA3IY9zvXXhJonIahF5VUR6AajqMuAdYLu3LFTVDeE7isjVIpIlIll5eXm1rkSpkD76tLh9MH48fPghHHZY3X+mMcY0c9E6GPsGkKGqg4HFwDMAInI40B/oiftwOE1ERobvrKqPqWqmqmamp6fXvRQhLfrU+P3uArHHH1/3n2eMMT4QSdBvBXqFPO7prSulqjtVtcB7+ARwtHd/IvCRqu5X1f3Am0DDJW+5oD/QYC9jjDHNSSRBvxzoKyJ9RCQRmAzMD91ARLqFPDwbCHbPfA2cIiLxIpKAOxBboesmaryumxwO5ZCf7GiwlzHGmOakxlE3qhoQkSnAQiAOmKmq60TkDiBLVecD14vI2UAA2AVc7u3+KnAasAZ3YPbfqvpG9KvhCQTYR1ty6cWA1Lcb7GWMMaY5iejMWFVdACwIWzc95P40YFol+xUDv65nGSMXCPA5RwLQ/7/PaLSXNcaYpsxfZ8YGAmygPwD9R/eMcWGMMaZp8FfQFxezgf4kxJfYiEpjjPH4K+i9Fv0RvfKJ99d0bcYYU2e+DPr+ffJjXRJjjGky/BX0xcV8QR8O710Y65IYY0yT4augLy4IECCB1q1jXRJjjGk6fBX0RQVuWuKExIabCdkYY5obXwV9Yb4FvTHGhPNV0BcVuvnnE5Mt6I0xJsiXQZ+Q6KtqGWNMvfgqEUu7bpKsRW+MMUG+CvqiInebmOSrahljTL34KhFLR90k+6paxhhTL75KxELvPKmEpLjYFsQYY5oQXwW9jboxxpiK/BX0Xh+9teiNMaZMREEvIuNEZKOIZIvILZU8f7mI5InISm+5MuS53iKySEQ2iMh6EcmIXvHLKyzwhlcmW9AbY0xQjZP5ikgcMAMYDeQCy0VkvqquD9v0ZVWdUsmPeBa4W1UXi0gboKS+ha5K6agbOxhrjDGlIknEEUC2quaoaiEwC5gQyQ8XkQFAvKouBlDV/ap6sM6lrUFhkeubT7DhlcYYUyqSROwBfBPyONdbF26SiKwWkVdFpJe37ghgj4jMFZEVIvJX7xtCOSJytYhkiUhWXl5erSsRVFTkHYxNrPOPMMYY34lW0/cNIENVBwOLgWe89fHASOAm4BjgUODy8J1V9TFVzVTVzPT09DoXoig4vDKhzj/CGGN8J5Kg3wr0Cnnc01tXSlV3qmqB9/AJ4Gjvfi6w0uv2CQCvAcPrV+SqlXbdWNAbY0ypSIJ+OdBXRPqISCIwGZgfuoGIdAt5eDawIWTfVBEJNtNPA8IP4kZNUcAFvXXdGGNMmRpH3ahqQESmAAuBOGCmqq4TkTuALFWdD1wvImcDAWAXXveMqhaLyE3A2yIiwKfA4w1TlZBx9NaiN8aYUjUGPYCqLgAWhK2bHnJ/GjCtin0XA4PrUcaIFQas68YYY8L5ahxiUcBVx7pujDGmjK+CvtALemvRG2NMGV8FfVGJBb0xxoTzV9Bbi94YYyrwVdAXFsfRimLibE4zY4wp5augLypuRYIEYl0MY4xpUnwX9IlSFOtiGGNMk+KroC8sjrMWvTHGhPFV0BeVWNeNMcaE81XQFwTiSbKuG2OMKcdfQV8ST1KrwlgXwxhjmhR/BX1xPEmtrEVvjDGhLOiNMcbnfBb0CRb0xhgTxl9BXxJPYisbdWOMMaEiCnoRGSciG0UkW0RuqeT5y0UkT0RWesuVYc+3E5FcEXkoWgWvTGFJPEkW9MYYU06NFx4RkThgBjAadw3Y5SIyX1XDLwn4sqpOqeLH3AksrVdJI1BQkkBSnHXdGGNMqEha9COAbO8C34XALGBCpC8gIkcDXYBFdSti5ApKEqxFb4wxYSIJ+h7ANyGPc7114SaJyGoReVVEegGISCvgXuCmepc0AtaiN8aYiqJ1MPYNIENVBwOLgWe89dcCC1Q1t7qdReRqEckSkay8vLw6F8IFfXGd9zfGGD+K5OLgW4FeIY97eutKqerOkIdPAPd4948HRorItUAbIFFE9qvqLWH7PwY8BpCZmam1qkGIgpJEa9EbY0yYSIJ+OdBXRPrgAn4ycGHoBiLSTVW3ew/PBjYAqOpFIdtcDmSGh3w0FWgCiXElDfXjjTGmWaox6FU1ICJTgIVAHDBTVdeJyB1AlqrOB64XkbOBALALuLwBy1ylQk0gKc4OxhpjTKhIWvSo6gJgQdi66SH3pwHTavgZTwNP17qEEVKFQk0kKd6C3hhjQvnmzNhCb9JKOxhrjDHl+SboCwrcbVK8Bb0xxoTyX9BbH70xxpQTUR99c5CSAjM6TuekbtaiN8aYUL4J+tat4dqUZ6DjqbEuijHGNCm+6boBoKQE4uJiXQpjjGlS/BX0xcXQyl9VMsaY+vJXKlqL3hhjKvBX0FuL3hhjKvBXKlqL3hhjKvBX0FuL3hhjKvBXKlqL3hhjKvBX0FuL3hhjKvBXKlqL3hhjKvBX0FuL3hhjKvBXKlqL3hhjKogo6EVknIhsFJFsEalwKUARuVxE8kRkpbdc6a0fKiLLRGSdiKwWkfOjXYFyrEVvjDEV1DipmYjEATOA0UAusFxE5qvq+rBNX1bVKWHrDgKXqupmEekOfCoiC1V1TzQKX4561xS3Fr0xxpQTSfN3BJCtqjmqWgjMAiZE8sNVdZOqbvbubwN2AOl1LWy1SryLgluL3hhjyokkFXsA34Q8zvXWhZvkdc+8KiK9wp8UkRFAIrClkueuFpEsEcnKy8uLsOhhir156C3ojTGmnGil4htAhqoOBhYDz4Q+KSLdgOeAX6pqSfjOqvqYqmaqamZ6eh0b/MEWvXXdGGNMOZEE/VYgtIXe01tXSlV3qqp3MT+eAI4OPici7YD/A25V1Y/qV9xqWIveGGMqFUkqLgf6ikgfEUkEJgPzQzfwWuxBZwMbvPWJwDzgWVV9NTpFroK16I0xplI1jrpR1YCITAEWAnHATFVdJyJ3AFmqOh+4XkTOBgLALuByb/fzgJOBjiISXHe5qq6MbjWwFr0xxlQhomvGquoCYEHYuukh96cB0yrZ73ng+XqWMTLWojfGmEr5p/lrLXpjjKmUf1IxMRHOPRf69o11SYwxpkmJqOumWWjfHmbPjnUpjDGmyfFPi94YY0ylLOiNMcbnLOiNMcbnLOiNMcbnLOiNMcbnLOiNMcbnLOiNMcbnLOiNMcbnRIOX4GsiRCQP+KoeP6IT8H2UitMctLT6gtW5pbA6184hqlrpBT2aXNDXl4hkqWpmrMvRWFpafcHq3FJYnaPHum6MMcbnLOiNMcbn/Bj0j8W6AI2spdUXrM4thdU5SnzXR2+MMaY8P7bojTHGhLCgN8YYn/NN0IvIOBHZKCLZInJLrMsTTSLypYisEZGVIpLlresgIotFZLN3m+atFxF50Ps9rBaR4bEtfWREZKaI7BCRtSHral1HEbnM236ziFwWi7pEqoo63y4iW733eqWInBny3DSvzhtFZGzI+mbxty8ivUTkHRFZLyLrROQGb71v3+dq6ty477OqNvsFiAO2AIcCicAqYECsyxXF+n0JdApbdw9wi3f/FuAv3v0zgTcBAY4DPo51+SOs48nAcGBtXesIdAByvNs0735arOtWyzrfDtxUybYDvL/rJKCP9/ce15z+9oFuwHDvfltgk1cv377P1dS5Ud9nv7ToRwDZqpqjqoXALGBCjMvU0CYAz3j3nwHOCVn/rDofAaki0i0WBawNVV0K7ApbXds6jgUWq+ouVd0NLAbGNXzp66aKOldlAjBLVQtU9QsgG/d332z+9lV1u6p+5t3/AdgA9MDH73M1da5Kg7zPfgn6HsA3IY9zqf6X2dwosEhEPhWRq711XVR1u3f/W6CLd99Pv4va1tEvdZ/idVXMDHZj4LM6i0gGMAz4mBbyPofVGRrxffZL0PvdSao6HBgPXCciJ4c+qe47n6/HybaEOnoeBg4DhgLbgXtjW5zoE5E2wBzgN6q6L/Q5v77PldS5Ud9nvwT9VqBXyOOe3jpfUNWt3u0OYB7ua9x3wS4Z73aHt7mffhe1rWOzr7uqfqeqxapaAjyOe6/BJ3UWkQRc4L2gqnO91b5+nyurc2O/z34J+uVAXxHpIyKJwGRgfozLFBUikiIibYP3gTHAWlz9gqMNLgNe9+7PBy71RiwcB+wN+Vrc3NS2jguBMSKS5n0VHuOtazbCjqdMxL3X4Oo8WUSSRKQP0Bf4hGb0ty8iAjwJbFDV+0Ke8u37XFWdG/19jvVR6WgtuCP0m3BHpm+NdXmiWK9DcUfYVwHrgnUDOgJvA5uBt4AO3noBZni/hzVAZqzrEGE9X8J9hS3C9T/+qi51BK7AHcDKBn4Z63rVoc7PeXVa7f0jdwvZ/lavzhuB8SHrm8XfPnASrltmNbDSW8708/tcTZ0b9X22KRCMMcbn/NJ1Y4wxpgoW9MYY43MW9MYY43MW9MYY43MW9MYY43MW9MYY43MW9MYY43P/H/uYMdLHYPkpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "maxInter_L2 = np.array([1,5,10,20,30,40,50,60,70,80,90,100,150,200,250,270,290,310,500,1000,1500,2000,2500])\n",
    "accuracyList_L2 = np.array([0.51465211, 0.5554351, 0.58936822, 0.57628835, 0.62843466, 0.580394,\n",
    "  0.61566472, 0.59599222, 0.62926156, 0.63654809, 0.61425936, 0.61453291,\n",
    "  0.61360813, 0.64144677, 0.62176674, 0.63253906, 0.63162808, 0.63394065,\n",
    "  0.63626325, 0.66871196, 0.67391806, 0.65594956, 0.67073468])\n",
    "\n",
    "maxInter_L1 = np.array([1,5,10,20,30,40,50,60,70,80,90,100,150,200,250,270,290,310,500,1000,1500,2000])\n",
    "accuracyList_L1 = np.array([0.53344752, 0.54818621, 0.57165945, 0.57777778, 0.59307234, 0.6095878,\n",
    "  0.5948855,  0.58531025, 0.61690319, 0.60148441, 0.62470544, 0.6055863,\n",
    "  0.63370851, 0.62611456, 0.62614468, 0.63656189, 0.63654683, 0.64810716,\n",
    "  0.64293243, 0.65881423, 0.67447644, 0.67157789])\n",
    "plt.plot(maxInter_L2, accuracyList_L2, 'r-', maxInter_L1, accuracyList_L1, 'b-')\n",
    "plt.legend(['logreg L2','logreg L1'])\n",
    "plt.savefig('Learning Curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3_skeleton.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
