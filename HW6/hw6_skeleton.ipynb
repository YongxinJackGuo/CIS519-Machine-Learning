{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dW8lnmtOexuV"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "installing dependencies\n",
    "\"\"\"\n",
    "!apt-get -qq -y install libnvtoolsext1 > /dev/null\n",
    "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
    "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
    "!pip -q install gym\n",
    "!pip -q install pyglet\n",
    "!pip -q install pyopengl\n",
    "!pip -q install pyvirtualdisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVdRtMI_k1Yi"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6930bc8140c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "from gym import wrappers\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()\n",
    "import os\n",
    "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n",
    "\n",
    "import matplotlib.animation\n",
    "import numpy as np\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJy7380Mk3-D"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.utils.tensorboard as tb\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils import data \n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "125rH3KKlB9G"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from datetime import datetime\n",
    "import glob, os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-o0B_1XB-roN"
   },
   "source": [
    "# Homework 6 - Imitation and Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0usZiaQMrvz5"
   },
   "source": [
    "### Getting to know OpenAI Gym. (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S9QbpaDF0RgX"
   },
   "source": [
    "\n",
    "We will be using the OpenAI Gym as our environment -- **we strongly recommend looking over the [\"Getting Started\" documentation](https://gym.openai.com/docs/) .**\n",
    "\n",
    "\n",
    "> A car is on a one-dimensional track, positioned between two \"mountains\". The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n",
    "\n",
    "\n",
    "![](https://user-images.githubusercontent.com/8510097/31701297-3ebf291c-b384-11e7-8289-24f1d392fb48.PNG)\n",
    "\n",
    "\n",
    ">The goal position is 0.5, the location of the flag on top of the hill.\n",
    "\n",
    ">Reward: -1 for each time step, until the goal position of 0.5 is reached.\n",
    "\n",
    ">Initialization: Random position from -0.6 to -0.4 with no velocity.\n",
    "\n",
    ">Episode ends when you reach 0.5 position, or if 200 timesteps are reached. (So failure to reach the flag will result in a reward of -200).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qfcl_TCJrzcY"
   },
   "outputs": [],
   "source": [
    "class ResizeObservation(gym.Wrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super(ResizeObservation, self).__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            shape = (shape, shape)\n",
    "        assert all(x > 0 for x in shape), shape\n",
    "        self.env = env\n",
    "        self.shape = tuple(shape)\n",
    "\n",
    "    def render(self):\n",
    "\n",
    "        from PIL import Image\n",
    "        obs = self.env.render(mode = 'rgb_array')\n",
    "        im = Image.fromarray(np.uint8(obs))\n",
    "        im = im.resize(self.shape)\n",
    "        return np.asarray(im)\n",
    "\n",
    "\n",
    "\n",
    "def dummy_policy(env, num_episodes):\n",
    "\n",
    "  '''\n",
    "  TODO: Fill in this function \n",
    "\n",
    "  Functionality: This should be executing a random policy sampled from the action space of the environment for num_episodes long\n",
    "                  and should be returning the mean reward over those episodes and the frames of the rendering recorded on the last episode \n",
    "  \n",
    "  Input: env, the MountainCar environment object \n",
    "         num_episodes, int, the total number of episodes you want to run this for \n",
    "\n",
    "  Returns: mean_reward, float, which is the mean_reward over num_episodes \n",
    "           frames, a list, which should contain elements of image dimensions (i.e RGB, with size that you specify), should have a length of the last episode that you record.\n",
    "\n",
    "  '''\n",
    "\n",
    "  return mean_reward, frames\n",
    "\n",
    "resize_observation_shape = 100\n",
    "env = gym.make('MountainCar-v0')\n",
    "env = ResizeObservation(env, resize_observation_shape)\n",
    "\n",
    "rew, frames = dummy_policy(env, 10)\n",
    "\n",
    "#### Video plotting code ######################\n",
    "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "patch = plt.imshow(frames[0])\n",
    "plt.axis('off')\n",
    "animate = lambda i: patch.set_data(frames[i])\n",
    "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rTQNkfsW-0ah"
   },
   "source": [
    "## Expert Reinforcement Learning Code - Q-Learning\n",
    "\n",
    "\n",
    "You are given the code for training a traditional Q-learning based agent. Please go through this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYnpmhGO_CIP"
   },
   "source": [
    "### Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4QgINqO_FWg"
   },
   "outputs": [],
   "source": [
    "def discretize(state, discretization, env):\n",
    "\n",
    "    env_minimum = env.observation_space.low\n",
    "    state_adj = (state - env_minimum)*discretization\n",
    "    discretized_state = np.round(state_adj, 0).astype(int)\n",
    "\n",
    "    return discretized_state\n",
    "\n",
    "\n",
    "def choose_action(epsilon, Q, state, env):\n",
    "    \"\"\"    \n",
    "    Choose an action according to an epsilon greedy strategy.\n",
    "    Args:\n",
    "        epsilon (float): the probability of choosing a random action\n",
    "        Q (np.array): The Q value matrix, here it is 3D for the two observation states and action states\n",
    "        state (Box(2,)): the observation state, here it is [position, velocity]\n",
    "        env: the RL environment \n",
    "        \n",
    "    Returns:\n",
    "        action (int): the chosen action\n",
    "    \"\"\"\n",
    "    action = 0\n",
    "    if np.random.random() < 1 - epsilon:\n",
    "        action = np.argmax(Q[state[0], state[1]]) \n",
    "    else:\n",
    "        action = np.random.randint(0, env.action_space.n)\n",
    "  \n",
    "    return action\n",
    "\n",
    "\n",
    "def update_epsilon(epsilon, decay_rate):\n",
    "    \"\"\"\n",
    "    Decay epsilon by the specified rate.\n",
    "    \n",
    "    Args:\n",
    "        epsilon (float): the probability of choosing a random action\n",
    "        decay_rate (float): the decay rate (between 0 and 1) to scale epsilon by\n",
    "        \n",
    "    Returns:\n",
    "        updated epsilon\n",
    "    \"\"\"\n",
    "  \n",
    "    epsilon *= decay_rate\n",
    "\n",
    "    return epsilon\n",
    "\n",
    "\n",
    "def update_Q(Q, state_disc, next_state_disc, action, discount, learning_rate, reward, terminal):\n",
    "    \"\"\"\n",
    "    \n",
    "    Update Q values following the Q-learning update rule. \n",
    "    \n",
    "    Be sure to handle the terminal state case.\n",
    "    \n",
    "    Args:\n",
    "        Q (np.array): The Q value matrix, here it is 3D for the two observation states and action states\n",
    "        state_disc (np.array): the discretized version of the current observation state [position, velocity]\n",
    "        next_state_disc (np.array): the discretized version of the next observation state [position, velocity]\n",
    "        action (int): the chosen action\n",
    "        discount (float): the discount factor, may be referred to as gamma\n",
    "        learning_rate (float): the learning rate, may be referred to as alpha\n",
    "        reward (float): the current (immediate) reward\n",
    "        terminal (bool): flag for whether the state is terminal\n",
    "        \n",
    "    Returns:\n",
    "        Q, with the [state_disc[0], state_disc[1], action] entry updated.\n",
    "    \"\"\"    \n",
    "    if terminal:        \n",
    "        Q[state_disc[0], state_disc[1], action] = reward\n",
    "\n",
    "    # Adjust Q value for current state\n",
    "    else:\n",
    "        delta = learning_rate*(reward + discount*np.max(Q[next_state_disc[0], next_state_disc[1]]) - Q[state_disc[0], state_disc[1],action])\n",
    "        Q[state_disc[0], state_disc[1],action] += delta\n",
    "  \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H9SoJsQqO-IM"
   },
   "source": [
    "#### Wrapper for Rendering the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0DawekRO9so"
   },
   "outputs": [],
   "source": [
    "class ResizeObservation(gym.Wrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super(ResizeObservation, self).__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            shape = (shape, shape)\n",
    "        assert all(x > 0 for x in shape), shape\n",
    "        self.env = env\n",
    "        self.shape = tuple(shape)\n",
    "\n",
    "    def render(self):\n",
    "\n",
    "        from PIL import Image\n",
    "        obs = self.env.render(mode = 'rgb_array')\n",
    "        im = Image.fromarray(np.uint8(obs))\n",
    "        im = im.resize(self.shape)\n",
    "        return np.asarray(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o1Raet4__WW_"
   },
   "source": [
    "### Main Q-learning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hw8Wz1Ik_Y-K"
   },
   "outputs": [],
   "source": [
    "def Qlearning(Q, discretization, env, learning_rate, discount, epsilon, decay_rate, max_episodes=5000):\n",
    "    \"\"\"\n",
    "    \n",
    "    The main Q-learning function, utilizing the functions implemented above.\n",
    "          \n",
    "    \"\"\"\n",
    "    reward_list = []\n",
    "    position_list = []\n",
    "    success_list = []\n",
    "    success = 0 # count of number of successes reached \n",
    "    frames = []\n",
    "  \n",
    "    for i in range(max_episodes):\n",
    "        # Initialize parameters\n",
    "        done = False # indicates whether the episode is done\n",
    "        terminal = False # indicates whether the episode is done AND the car has reached the flag (>=0.5 position)\n",
    "        tot_reward = 0 # sum of total reward over a single\n",
    "        state = env.reset() # initial environment state\n",
    "        state_disc = discretize(state,discretization,env)\n",
    "\n",
    "        while done != True:                 \n",
    "            # Determine next action \n",
    "            action = choose_action(epsilon, Q, state_disc, env)                                      \n",
    "            # Get next_state, reward, and done using env.step(), see http://gym.openai.com/docs/#environments for reference\n",
    "            if i==1 or i==(max_episodes-1):\n",
    "              frames.append(env.render())\n",
    "            next_state, reward, done, _ = env.step(action) \n",
    "            # Discretize next state \n",
    "            next_state_disc = discretize(next_state,discretization,env)\n",
    "            # Update terminal\n",
    "            terminal = done and next_state[0]>=0.5\n",
    "            # Update Q\n",
    "            Q = update_Q(Q,state_disc,next_state_disc,action,discount,learning_rate, reward, terminal)  \n",
    "            # Update tot_reward, state_disc, and success (if applicable)\n",
    "            tot_reward += reward\n",
    "            state_disc = next_state_disc\n",
    "\n",
    "            if terminal: success +=1 \n",
    "            \n",
    "        epsilon = update_epsilon(epsilon, decay_rate) #Update level of epsilon using update_epsilon()\n",
    "\n",
    "        # Track rewards\n",
    "        reward_list.append(tot_reward)\n",
    "        position_list.append(next_state[0])\n",
    "        success_list.append(success/(i+1))\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Episode: ', i+1, 'Average Reward over 100 Episodes: ',np.mean(reward_list))\n",
    "            reward_list = []\n",
    "                \n",
    "    env.close()\n",
    "    \n",
    "    return Q, position_list, success_list, frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2YdXsm9_hwA"
   },
   "source": [
    "### Define Params and Launch Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLCZfiw6_mtd"
   },
   "outputs": [],
   "source": [
    "# Initialize Mountain Car Environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "env = ResizeObservation(env,100) #Resize observations\n",
    "\n",
    "env.seed(42)\n",
    "np.random.seed(42)\n",
    "env.reset()\n",
    "\n",
    "# Parameters    \n",
    "learning_rate = 0.2 \n",
    "discount = 0.9\n",
    "epsilon = 0.8 \n",
    "decay_rate = 0.95\n",
    "max_episodes = 5000\n",
    "discretization = np.array([10,100])\n",
    "\n",
    "\n",
    "#InitQ\n",
    "num_states = (env.observation_space.high - env.observation_space.low)*discretization\n",
    "#Size of discretized state space \n",
    "num_states = np.round(num_states, 0).astype(int) + 1\n",
    "# Initialize Q table\n",
    "Q = np.random.uniform(low = -1, \n",
    "                      high = 1, \n",
    "                      size = (num_states[0], num_states[1], env.action_space.n))\n",
    "\n",
    "# Run Q Learning by calling your Qlearning() function\n",
    "Q, position, successes, frames = Qlearning(Q, discretization, env, learning_rate, discount, epsilon, decay_rate, max_episodes)\n",
    "\n",
    "np.save('./expert_Q.npy',Q) #Save the expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kq32GUwfS0fj"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJ94bDA5S2Yv"
   },
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ur4GYN0sScb0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "plt.plot(successes)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('% of Episodes with Success')\n",
    "plt.title('% Successes')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "p = pd.Series(position)\n",
    "ma = p.rolling(3).mean()\n",
    "plt.plot(p, alpha=0.8)\n",
    "plt.plot(ma)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Car Final Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NjnH4M1oS4nt"
   },
   "source": [
    "#### Agent's Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuKZrmD9S6tr"
   },
   "outputs": [],
   "source": [
    "#### Video plotting code #####################\n",
    "deep_frames = []\n",
    "for obs in frames:\n",
    "  im = Image.fromarray(np.uint8(obs))\n",
    "  im = im.resize((600,400))\n",
    "  deep_frames.append(np.asarray(im))\n",
    "\n",
    "plt.figure(figsize=(deep_frames[0].shape[1] / 72.0, deep_frames[0].shape[0] / 72.0), dpi = 72)\n",
    "patch = plt.imshow(deep_frames[0])\n",
    "plt.axis('off')\n",
    "animate = lambda i: patch.set_data(deep_frames[i])\n",
    "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(deep_frames), interval = 50)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZ-fUyb0AY4N"
   },
   "source": [
    "### Generate Expert Trajectories (TODO):\n",
    "\n",
    "Using the Q-learning agent above, please complete this block of code to generate expert trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5bJ_tJ2kFOhu"
   },
   "source": [
    "#### Get actions from expert for a specific observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZUC2YOz-FHcL"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_expert_action(Q,discretization,env,state):\n",
    "  '''\n",
    "  TODO: Implement this function\n",
    "\n",
    "  NOTE: YOU WILL BE USING THIS FUNCTION FOR THE IMITATION LEARNING SECTION AS WELL \n",
    "\n",
    "  Functionality: For a given state, returns the action that the expert would take \n",
    "\n",
    "  Input: Q value , numpy array\n",
    "         the discretization \n",
    "         env , the environment\n",
    "         state, (Box(2,)): the observation space state, here it is [position, velocity]\n",
    "\n",
    "  Returns: action, has dimensions of the action space and type of action space\n",
    "  '''\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RaOtKX9yFkbe"
   },
   "source": [
    "#### Generate Expert Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5bHmGmamAgZ2"
   },
   "outputs": [],
   "source": [
    "def generate_expert_trajectories(Q, discretization, env, num_episodes=150, data_path='./data'):\n",
    "\n",
    "  '''\n",
    "  TODO: Implement this function\n",
    "\n",
    "  Functionality: Execute Expert Trajectories and Save them under the folder of data_path/\n",
    "\n",
    "  Input: Q value , numpy array\n",
    "         the discretization \n",
    "         env , the environment\n",
    "         num_episodes, int, which is used to denote number of expert trajectories to store \n",
    "         \n",
    "  Returns: total_samples, int, which denotes the total number of samples that were stored\n",
    "  '''\n",
    "\n",
    "\n",
    "  episode_dict['observations'] = episode_observations\n",
    "  episode_dict['actions'] = episode_actions\n",
    "  import os\n",
    "  if not os.path.exists(data_path):\n",
    "     os.makedirs(data_path)\n",
    "  np.savez_compressed(data_path+'/episode_number'+'_'+str(i)+'.npz',**episode_dict) #where i can be the episode number that you save\n",
    "\n",
    "  return total_samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QVq79h0HFucN"
   },
   "source": [
    "#### Launch code for generating trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRl9qTlhF1Vo"
   },
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "data_path = './data'\n",
    "\n",
    "\n",
    "total_samples = generate_expert_trajectories(Q,discretization,env,num_episodes,save_images, data_path) ## Generate trajectories. Use Q, discretization and env by running the previous section\n",
    "\n",
    "print('---------Total Samples Recorded were --------', total_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T1nc4RFkkf6c"
   },
   "source": [
    "## Imitation Learning\n",
    "\n",
    "Using the trajectories that you collected from the expert above, you will work on imitation learning agents in the code sections below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lu0OBrQ_JpU5"
   },
   "source": [
    "### Working with Data (TODO)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJO2VDHjJvCU"
   },
   "source": [
    "#### Loading Initial Expert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SwbBU5lEy9vR"
   },
   "outputs": [],
   "source": [
    "def load_initial_data(args):\n",
    "  '''\n",
    "  TODO: Fill this function\n",
    "\n",
    "  Functionality: Reads data from directory and converts them into numpy arrays of observations and actions\n",
    "\n",
    "  Input arguments: args, an object with set of parameters and objects that you can treat as an attribute dictionary. Access elements with args.element_you_want_to_access \n",
    "  \n",
    "  Returns: training_observations: numpy array, of shape (B,dim_of_observation), where B is total number of samples that you select\n",
    "           training_actions: numpy array, of shape (B,dim_of_action), where B is total number of samples that you select\n",
    "\n",
    "  '''\n",
    "\n",
    "  return training_observations, training_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilEPYbBSNDCa"
   },
   "source": [
    "#### Convert numpy arrays to a Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJBqhJpaPzKa"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(args, observations, actions, batch_size=64, data_transforms=None, num_workers=0):\n",
    "  '''\n",
    "  TODO: Fill this function fully \n",
    "\n",
    "  Functionality: Converts numpy arrays to dataloaders. \n",
    "  \n",
    "  Inputs: args, an object with set of parameters and objects that you can treat as an attribute dictionary. Access elements with args.element_you_want_to_access \n",
    "          observations, numpy array, of shape (B,dim_of_observation), where B is number of samples \n",
    "          actions, numpy array, of shape (B,dim_of_action), where B is number of samples \n",
    "          batch_size, int, which you can play around with, but is set to 64 by default. \n",
    "          data_transforms, whatever transformations you want to make to your data.\n",
    "\n",
    "  Returns: dataloader  \n",
    "          \n",
    "\n",
    "  '''\n",
    "\n",
    "  return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMl19NtzXsjS"
   },
   "source": [
    "#### Process Individual Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Om0xN3o0Xs0d"
   },
   "outputs": [],
   "source": [
    "def process_individual_observation(args,observation):\n",
    "  '''\n",
    "  TODO: Fill this function fully \n",
    "\n",
    "  Functionality: Converts individual observations according to the pre-processing that you want  \n",
    "  \n",
    "  Inputs: args, an object with set of parameters and objects that you can treat as an attribute dictionary. Access elements with args.element_you_want_to_access \n",
    "          observations, shape (dim_of_observation)\n",
    "\n",
    "  Returns: data, processed observation that can be fed into the model\n",
    "  '''\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9JbPKbIuNIXz"
   },
   "source": [
    "### Defining Networks (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QwuwPCDMNK0-"
   },
   "source": [
    "#### Define your network for working from States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5WWUc3JQI5Y"
   },
   "outputs": [],
   "source": [
    "class StatesNetwork(nn.Module):\n",
    "  '''\n",
    "  TODO: Implement this class\n",
    "  '''\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Your code here\n",
    "        \"\"\"\n",
    "    \n",
    "    def forward(self, x):    \n",
    "        \"\"\"\n",
    "        Your code here\n",
    "        @x: torch.Tensor((B,dim_of_observation))\n",
    "        @return: torch.Tensor((B,dim_of_actions))\n",
    "        \"\"\"\n",
    "\n",
    "        return forward_pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67PrpWq1VbRn"
   },
   "source": [
    "### Training the model (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWTl3W5PQsED"
   },
   "outputs": [],
   "source": [
    "def train_model(args):\n",
    "\n",
    "  '''\n",
    "  TODO: Fill in the entire train function\n",
    "\n",
    "  Functionality: Trains the model. How you train this is upto you. \n",
    "\n",
    "  Input: args, an object with set of parameters and objects that you can treat as an attribute dictionary. Access elements with args.element_you_want_to_access \n",
    "\n",
    "  Returns: The trained model \n",
    "\n",
    "  '''\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AKqAIeq6XXg3"
   },
   "source": [
    "### DAgger (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HIQFJ4qVfWmW"
   },
   "source": [
    "#### Get the expert trajectory for imitating agent's observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ByPXNJ2YRP_j"
   },
   "outputs": [],
   "source": [
    "def execute_dagger(args):\n",
    "\n",
    "    '''\n",
    "  TODO: Implement this function\n",
    "\n",
    "  Functionality: Collect expert labels for the observations seen by the imitation learning agent \n",
    "  \n",
    "  Input: args, an object with set of parameters and objects that you can treat as an attribute dictionary. Access elements with args.element_you_want_to_access \n",
    "         \n",
    "  Returns: imitation_observations, a numpy array that has dimensions of (episode_length,dim_of_observation)\n",
    "           expert_actions, a numpy array that has dimensions of (episode_length,dim_of_action)\n",
    "  '''\n",
    "\n",
    "          \n",
    "    return imitation_observations, expert_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yzknvrunfeR_"
   },
   "source": [
    "#### Aggregate new rollout to the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j1KBUAV4RQxd"
   },
   "outputs": [],
   "source": [
    "def aggregate_dataset(training_observations, training_actions, imitation_states, expert_actions):\n",
    "\n",
    "    '''\n",
    "  TODO: Implement this function\n",
    "\n",
    "  Functionality: Adds new expert labeled rollout to the overall dataset\n",
    "\n",
    "  Input: training_observations, a numpy array that has dimensions of (dataset_size,dim_of_observation)\n",
    "         training_actions, a numpy array that has dimensions of (dataset_size,dim_of_action)\n",
    "         imitation_observations, a numpy array that has dimensions of (episode_length,dim_of_observation)\n",
    "         expert_actions, a numpy array that has dimensions of (episode_length,dim_of_action)\n",
    "\n",
    "  Returns: training_observations, a numpy array that has dimensions of (updated_dataset_size,dim_of_observation)\n",
    "           training_actions, a numpy array that has dimensions of (updated_dataset_size,dim_of_action)\n",
    "  '''\n",
    "\n",
    "\n",
    "  return training_observations, training_actions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1iwZO4bjS5Sk"
   },
   "source": [
    "### Utility \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OXo6py_jrA4K"
   },
   "source": [
    "#### Code for prediction of the network and calculating the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBwn7zcIQnm7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    outputs_idx = outputs.max(1)[1].type_as(labels)\n",
    "    return outputs_idx.eq(labels).float().mean()\n",
    "\n",
    "def predict(model, inputs, device='cpu'):\n",
    "    inputs = inputs.to(device)\n",
    "    logits = model(inputs)\n",
    "    return F.softmax(logits, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2xMbD3KcULy7"
   },
   "source": [
    "#### Wrapper for Rendering the environment \n",
    "\n",
    "Same code that was used in the Q-learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5cVe-se9UTlZ"
   },
   "outputs": [],
   "source": [
    "class ResizeObservation(gym.Wrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super(ResizeObservation, self).__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            shape = (shape, shape)\n",
    "        assert all(x > 0 for x in shape), shape\n",
    "        self.env = env\n",
    "        self.shape = tuple(shape)\n",
    "\n",
    "    def render(self):\n",
    "\n",
    "        from PIL import Image\n",
    "        obs = self.env.render(mode = 'rgb_array')\n",
    "        im = Image.fromarray(np.uint8(obs))\n",
    "        im = im.resize(self.shape)\n",
    "        return np.asarray(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IeoRSlHKfidU"
   },
   "source": [
    "### Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJPYgakcRin7"
   },
   "outputs": [],
   "source": [
    "def test_model(args, record_frames=False):\n",
    "\n",
    "    '''\n",
    "  Functionality: Should take your model and run it for a complete episode (model should either not finish the game in 200 steps or finish the game). Record stats\n",
    "\n",
    "  Input: args, an object with set of parameters and objects that you can treat as an attribute dictionary. Access elements with args.element_you_want_to_access \n",
    "         record_frames, Boolean. Denotes if you want to record frames to display them as video.\n",
    "\n",
    "  Returns: final_position, The final position of the car when the episode ended\n",
    "           success, Boolean, denotes if the episode was a success or not\n",
    "           frames, a list of frames that have been rendered throughout the episode. Should have a length of the total episode length\n",
    "           episode_reward, float, denotes the total rewards obtained while executing this episode\n",
    "  '''\n",
    "\n",
    "    frames = []\n",
    "    env = args.env \n",
    "    model = args.model\n",
    "    state = env.reset()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    episode_reward = 0\n",
    "\n",
    "    success = False\n",
    "    done = False \n",
    "\n",
    "    while not done:\n",
    "\n",
    "        observation = state\n",
    "\n",
    "        data = preprocess_individual_observation(args,observation)\n",
    "        logit = model(data)\n",
    "        action = torch.argmax(logit).item()\n",
    "\n",
    "        if record_frames: #You can change the rate of recording frames as you like\n",
    "            frames.append(env.render())\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action) \n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:    \n",
    "            if next_state[0] >= 0.5:\n",
    "                success = True\n",
    "            final_position = next_state[0]\n",
    "            return final_position,success, frames, episode_reward\n",
    "        else:\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hfAkdhUdfqm9"
   },
   "source": [
    "### Main Imitation Learning Method (TODO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A8b-ZKT5Rjt7"
   },
   "outputs": [],
   "source": [
    "def imitate(args):\n",
    "    '''\n",
    "  TODO: Implement this function\n",
    "\n",
    "  Input: args, an object with set of parameters and objects that you can treat as an attribute dictionary. Access elements with args.element_you_want_to_access \n",
    "\n",
    "  Functionality: For a given set of args, performs imitation learning as desired. \n",
    "\n",
    "  Returns: final_positions, A list of final positions achieved by the model during every time it is tested. Should have a list length of args.max_dagger_iterations\n",
    "           success_history, A list of success percentage achieved by the model during every time it is tested. Should have a list length of args.max_dagger_iterations\n",
    "           reward_history, A list of episode rewards achieved by the model during every time it is tested. Should have a list length of args.max_dagger_iterations\n",
    "           frames, A list of video frames of the model executing its policy every time it is tested, can choose to not record. Should have a length of the number of times you chose to record frames\n",
    "           args, an object with set of parameters and objects that you can treat as an attribute dictionary. Access elements with args.element_you_want_to_access\n",
    "  '''\n",
    "\n",
    "  return final_positions, success_history, frames, reward_history, args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nWo7-F3dgX-Q"
   },
   "source": [
    "### Launch Imitation Learning (TODO)\n",
    "Define Args and Launch 'imitate'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vIk8kinmRl2g"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Expand the attributes of Args as you please. \n",
    "\n",
    "But please maintain the ones given below, i.e: You should be using the ones given below. Fill them out.\n",
    "\n",
    "Some of these are already filled out for you. \n",
    "'''\n",
    "\n",
    "\n",
    "##TODO: Fill in the given attributes (you should use these in your code), and add to them as you please.\n",
    "class Args(object):\n",
    "  pass\n",
    "\n",
    "args = Args();\n",
    "args.datapath = \n",
    "args.env = \n",
    "args.do_dagger = \n",
    "args.max_dagger_iterations = \n",
    "if not args.do_dagger:\n",
    "  assert args.max_dagger_iterations==1\n",
    "args.record_frames = \n",
    "args.initial_episodes_to_use = \n",
    "args.model = StatesNetwork(args.env)\n",
    "args.num_epochs = \n",
    "args.Q = np.load('./expert_Q.npy',allow_pickle=True)\n",
    "args.discretization = np.array([10,100])\n",
    "\n",
    "\n",
    "positions, successes, frames, reward_history, args = imitate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJNvSqnUJTTD"
   },
   "source": [
    "### Average Performance Metrics\n",
    "\n",
    "Use this function to see how well your agent is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "09qKRfmAJSYT"
   },
   "outputs": [],
   "source": [
    "def get_average_performance(args, run_for=1000):\n",
    "\n",
    "  final_positions = 0\n",
    "  successes = 0\n",
    "  rewards = 0\n",
    "\n",
    "  for ep in range(run_for):\n",
    "    pos, success, _, episode_rewards = test_model(args, record_frames=False)   #test imitation policy\n",
    "    final_positions += pos \n",
    "    rewards += episode_rewards\n",
    "    if success:\n",
    "      successes += 1\n",
    "    print('Running Episode: ',ep,' Success: ', success)\n",
    "    average_final_positions = final_positions/(ep+1)\n",
    "    average_success_rate = 100*(successes/(ep+1))\n",
    "    average_episode_rewards = rewards/(ep+1)\n",
    "\n",
    "  return average_final_positions, average_success_rate, average_episode_rewards \n",
    "\n",
    "\n",
    "final_pos, succ_rate, ep_rwds = get_average_performance(args)\n",
    "\n",
    "print('Average Final Position achieved by the Agent: ',final_pos)\n",
    "print('Average Success Rate achieved by the Agent: ',succ_rate)\n",
    "print('Average Episode Reward achieved by the Agent: ',ep_rwds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cXDS-0Jjrj4V"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgbpNa3erm_r"
   },
   "source": [
    "#### Plotting code\n",
    "\n",
    "Use the code below to make plots to see how well your agent did as it trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ItUKvgSeRo3i"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "plt.plot(successes)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('% of Episodes with Success')\n",
    "plt.title('% Successes')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "p = pd.Series(positions)\n",
    "ma = p.rolling(3).mean()\n",
    "plt.plot(p, alpha=0.8)\n",
    "plt.plot(ma)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Car Final Position')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(reward_history)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Rewards Achieved')\n",
    "plt.title('Episode Rewards')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "by8gqpior2iz"
   },
   "source": [
    "#### Make a video!\n",
    "\n",
    "Using the frames that you recorded in ``` frames ```, Run the code below to display a video that you can use to see how well your agent is doing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jg7OQix9RpWu"
   },
   "outputs": [],
   "source": [
    "#### Video plotting code #####################\n",
    "deep_frames = []\n",
    "for f in frames:\n",
    "  deep_frames += f\n",
    "plt.figure(figsize=(deep_frames[0].shape[1] / 72.0, deep_frames[0].shape[0] / 72.0), dpi = 72)\n",
    "patch = plt.imshow(deep_frames[0])\n",
    "plt.axis('off')\n",
    "animate = lambda i: patch.set_data(deep_frames[i])\n",
    "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(deep_frames), interval = 50)\n",
    "HTML(ani.to_jshtml())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw6_skeleton.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
